\documentclass{tufte-handout}
%\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{semantic}
\usepackage{wrapfig}
\usepackage{xypic}
\usepackage{marginfix}
%\usepackage{exercise}
\usepackage[noanswer]{exercise}
%\usepackage[answerdelayed]{exercise}
\usepackage{upgreek}

\renewcommand{\ExerciseHeader}{\noindent \textbf{
                \ExerciseName\;\ExerciseHeaderNB\ExerciseHeaderTitle
                \ExerciseHeaderOrigin}}

\renewcommand{\AnswerHeader}{\medskip{\noindent\textbf{Answer of \ExerciseName\ \ExerciseHeaderNB \\[1ex]}\smallskip}}

%\newcommand{\BR}[1]{\llbracket #1 \rrbracket}
\newcommand{\defeq}[0]{\overset{\mathrm{def}}{=}}
\newcommand{\BR}[1]{(#1)}
\newcommand{\SEM}[1]{\llbracket #1 \rrbracket}
\newcommand{\LAM}[1]{\lambda #1.\,}
\newcommand{\MLAM}[1]{\boldsymbol\uplambda #1.\,}
\newcommand{\SET}[1]{\mathcal{P}(#1)}
\newcommand{\FSET}[1]{\mathcal{P}_f(#1)}
\newcommand{\APP}[0]{\,}
\newcommand{\ASSIGN}[2]{#1 \mathrel{:=} #2}
\newcommand{\SEQ}[2]{#1 \mathrel{;} #2}
\newcommand{\OUTPUT}[1]{\mathtt{put}(#1)\texttt{;}}
\newcommand{\SKIP}[0]{\mathtt{skip}}
\newcommand{\INPUT}[0]{\mathtt{get}()}
\newcommand{\IF}[3]{\mathtt{if}\,#1\,\mathtt{then}\,#2\,\mathtt{else}\,#3}
\newcommand{\If}[3]{\mathit{if}\,#1\,\mathit{then}\,#2\,\mathit{else}\,#3}
\newcommand{\WHILE}[2]{\mathtt{while}\,#1\,\mathtt{do}\,#2}
\newcommand{\TRUE}[0]{\mathtt{true}}
\newcommand{\FALSE}[0]{\mathtt{false}}
\newcommand{\of}[0]{\!:\!}
\newcommand{\by}[0]{\!:=\!}
\newcommand{\pto}[0]{\rightharpoonup}
\newcommand{\BIN}[0]{\mathit{Bin}}
\newcommand{\NAT}[0]{\mathbb{N}}
\newcommand{\NATTY}[0]{\mathit{Nat}}
\newcommand{\UNITTY}[0]{\mathit{Unit}}
\newcommand{\FNAT}[0]{\mathbb{N}_{64}}
\newcommand{\BOOL}[0]{\mathbb{B}}
\newcommand{\BOOLTY}[0]{\mathit{Bool}}
\newcommand{\VAR}[0]{\mathbb{X}}
\newcommand{\EXP}[0]{\mathbb{E}}
\newcommand{\CND}[0]{\mathit{Cnd}}
\newcommand{\CMD}[0]{\mathit{Cmd}}
\newcommand{\STORE}[0]{\mathit{Store}}
\newcommand{\COND}[3]{#1\,\texttt{?}\,#2\,\texttt{:}\,#3}
\newcommand{\ext}[3]{#3(#1{\mapsto}#2)}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}%[section]
\newtheorem{conjecture}{Conjecture}%[section]
\newtheorem{example}{Example}%[section]
\newtheorem{remark}{Remark}

\title{Lecture Notes on Denotational Semantics}
\author{Jeremy G. Siek}

\begin{document}

\maketitle

\tableofcontents

\clearpage


\section{Binary Arithmetic}
\label{sec:binary-arithmetic}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Schmidt:1986vn} Chapter 4 \\
Exercises: & 4.2, 4.6
\end{tabular}
}

Borrowing and combining elements from Chapter 4 of
\citet{Schmidt:1986vn}, we consider the language of binary arithmetic
specified in Figure~\ref{fig:binary-arithmetic}.  The Syntax defines
the form of the programs and the Semantics specifies the behavior of
running the program. In this case, the behavior is simply to output a
number (in decimal). In our grammar for binary numerals, $\epsilon$
represents the empty string of digits and $bd$ is a number whose
least-significant digit is $d$ and whose more-significant digits are
given by $b$. (This is the reverse of \citet{Schmidt:1986vn} but the
same direction as \citet{Stoy:1977aa}). We place superscripts on our
binary numerals to enfore a bound on their maximum length, restricting
them 64 bits. We ommit the superscripts when their length is not of
interest.

The main purpose of a semantics is communicate in a precise way with
other people, primarily language implementers and programmers.  Thus,
it is incredibly important for the semantics to be written in a way
that will make it most easily understood, while still being completely
precise.  Here we have chosen to give the semantics of binary
arithmetic in terms of decimal numbers because people are generally
much more familiar with decimal numbers. We write $\FNAT$ for the set
of numbers from $0$ to $2^{64}-1$.

\begin{figure}[tbp]
\noindent Syntax
\[
\begin{array}{lrl}
 \text{digit}& d ::=& 0 \mid 1 \\
 \text{binary numeral}& b^n \in \BIN^n ::=& \epsilon^n \mid (b^nd)^{n+1} \\
 \text{expression}& e \in \EXP ::=& b^{64} \mid e + e \mid e \times e
\end{array}
\]
Semantics\\
\begin{minipage}{0.4\textwidth}
  \begin{align*}
 N : & \; \BIN^{64} \to \FNAT \\
 N\BR{ \epsilon } &= 0 \\
 N\BR{ b d } &= 2 N\BR{ b } + d \\[1ex]
\end{align*}
\end{minipage}
\begin{minipage}{0.6\textwidth}
  \begin{align*}
E : & \; \EXP \pto \FNAT \\
E\BR{ b } &= N\BR{ b } \\
E\BR{ e_1 + e_2 } &=
\begin{cases}
   n_1 + n_2 & \text{if } n_1 + n_2 < 2^{64}\\
   \mathrm{undefined} & \text{otherwise}
\end{cases}\\
 \text{where } & n_1 = E\BR{ e_1 }, n_2 = E\BR{ e_2 } \\
 E\BR{ e_1 \times e_2 } &= 
\begin{cases}
   n_1 n_2 & \text{if } n_1 n_2 < 2^{64}\\
   \mathrm{undefined} & \text{otherwise}
\end{cases}\\
 \text{where } & n_1 = E\BR{ e_1 }, n_2 = E\BR{ e_2 } 
\end{align*}
\end{minipage}
\caption{Language of binary arithmetic}
\label{fig:binary-arithmetic}
\end{figure}

When writing down a semantics, one is often tempted to consider the
efficiency of a semantics, as if it were an implementation. Indeed, it
would be straightforward to transcribe the definitions of $E$ and $N$
in Figure~\ref{fig:binary-arithmetic} into your favorite programming
language and thereby obtain an interpreter. All other things being
equal, it is fine to prefer a semantics that is suggestive of an
implementation, but one should prioritize ease of understanding first.
As a result, some semantics that we study may be more declarative in
nature. This is not to say that one should not consider the efficiency
of implementations when designing a language.  Indeed, the semantics
should be co-designed with implementations to avoid accidental designs
that preclude the desired level of efficiency.  Thus, a recurring
theme of these notes will be to consider implementations of languages
alongside their semantics.



Figure~\ref{fig:interp-binary} presents an interpreter for binary
arithmetic. This interpreter, in a way reminiscent of real computers,
operates on the binary numbers directly. The auxiliary functions
$\mathit{add}$ and $\mathit{mult}$ implement the algorithms you
learned in grade school, but for binary instead of decimals.

\begin{figure}[tbp]
\noindent Interpreter 
\begin{align*}
  I : & \; \EXP \pto \BIN^{64} \\
I\BR{ b } &= b \\
I\BR{ e_1 + e_2 } &=
  \mathit{add}(I\BR{ e_1 }, I\BR{ e_2 },0) \\
I\BR{ e_1 \times e_2 } &=
  \mathit{mult}(I\BR{ e_1 }, I\BR{ e_2 })
\end{align*}
%% Convert natural number to binary
%% \begin{align*}
%%  \mathit{binary}(n) &=
%%   \begin{cases}
%%      n & \text{if } n < 2 \\
%%      \mathit{binary}(n/2)\, (n \mathrel{\mathrm{mod}} 2)   &  \text{otherwise}
%%   \end{cases}
%% \end{align*}
Auxiliary Functions
\begin{align*}
  \mathit{bin}^2(n) &=
  \begin{cases}
     00 &  \text{if } n = 0 \\
     01 &  \text{if } n = 1 \\
     10 &  \text{if } n = 2 \\
     11 &  \text{if } n = 3
  \end{cases}
  \\[2ex]
  %% \mathit{add3}(d_1, d_2, d_3) &= 
  %% \text{let } n = d_1 + d_2 + d_3 \text{ in } \\
  %% &\quad \begin{cases}
  %%    00 &  \text{if } n = 0 \\
  %%    01 &  \text{if } n = 1 \\
  %%    10 &  \text{if } n = 2 \\
  %%    11 &  \text{if } n = 3
  %% \end{cases}\\
 \mathit{add}(\epsilon, \epsilon, c) &= 
   \begin{cases}
      \epsilon  & \text{if } c = 0 \\
      \epsilon 1  & \text{if } c = 1
   \end{cases}
   \\
  \mathit{add}(b_1 d_1 , b_2 d_2, c) &=  b_3\,d_3
  \text{ if } \mathit{bin}^2(d_1+ d_2+ c) = c' d_3,\\
  & \qquad  \mathit{add}(b_1, b_2, c') = b^n_3, \text{ and } n < 64 \\
\mathit{add}(b_1 d_1 , \epsilon, c) &= 
  \mathit{add}(b_1 d_1 , \epsilon 0, c) \\
\mathit{add}(\epsilon, b_2 d_2, c) &= 
\mathit{add}( \epsilon 0, b_2 d_2,c)
\\[2ex]
\mathit{mult}(b_1, \epsilon) &= \epsilon \\
\mathit{mult}(b_1, b_2 0) &= b_3 0
  \text{ if } \mathit{mult}(b_1,b_2) = b^n_3 \text{ and } n < 64 \\
 \mathit{mult}(b_1, b_2 1) &= 
   \mathit{add}(b_1, b_30, 0) \\
  & \qquad \text{ if } \mathit{mult}(b_1,b_2) = b^n_3 \text{ and } n < 64
\end{align*}
\caption{Binary arithmetic interpreter.}
\label{fig:interp-binary}
\end{figure}


\begin{Exercise}
\label{ex:bit-add}
Prove that 
$N (\mathit{bin}^2(n)) = n$ for any $n < 4$.
\end{Exercise}
\begin{Answer}
\begin{tabular}{l|c|c} 
  $n$ & $\mathit{bin}^2(n)$ & $N(\mathit{bin}^2(n))$ \\ \hline
  $0$ & $00$ & $2\cdot 0 + 0 = 0$ \\
  $1$ & $01$ & $2\cdot 0 + 1 = 1$\\
  $2$ & $10$ & $2\cdot 1 + 0 = 2$ \\
  $3$ & $11$ & $2\cdot 1 + 1 = 3$ 
\end{tabular} 
\end{Answer}

\begin{Exercise}
  \label{ex:add}
  Prove that $N(\mathit{add}(b_1,b_2,c)) = N(b_1) + N(b_2) + c$.
\end{Exercise}
\begin{Answer}
The proof is by induction on $\mathit{add}$.
\begin{itemize}
\item Case $N(\mathit{add}(\epsilon, \epsilon, 0)) = N\BR{\epsilon} = 0
       = N\BR{ \epsilon} + N\BR{ \epsilon} + 0$
\item Case $N(\mathit{add}(\epsilon, \epsilon, 1)) = N\BR{\epsilon 1} = 1
       = N\BR{\epsilon} + N\BR{\epsilon} + 1$
\item Case $\mathit{add}(b'_1 d_1 , b'_2  d_2, c) 
    = \mathit{add}(b_1, b_2, c')d_3$ \\
  where $\mathit{add3}(d_1, d_2, c) = c'd_3$. 
    \begin{align*}
    N(\mathit{add}(b'_1 d_1 , b'_2  d_2, c)) 
      &= N(\mathit{add}(b'_1, b'_2, c')d_3) \\
      &= N(\mathit{add}(b'_1, b'_2, c')) \times 2 + d_3 \\
      &= (N(b'_1) + N(b'_2) + c') \times 2 + d_3\\
      &= 2 N(b'_1) + 2 N(b'_2) + c' \times 2 + d_3 \\
      &= 2 N(b'_1) + 2 N(b'_2) + N\BR{c' d_3} \\
      &= 2 N(b'_1) +  d_1 + 2 N(b'_2) +  d_2 + c 
         & \text{by Ex.~\ref{ex:bit-add}}\\
      &= N \BR{b'_1 d_1} + N \BR{b'_2 d_2} + c
    \end{align*}

\item Case $\mathit{add}(b_1 d_1 , \epsilon, c) = 
  \mathit{add}(b_1 d_1 , \epsilon 0, c)$
  \begin{align*}
    N(\mathit{add}(b_1 d_1 , \epsilon, c))
    & = N\mathit{add}(b_1 d_1 , \epsilon 0, c) \\
    & = N \BR{b_1d_1} + N \BR{\epsilon 0} + c & \text{by I.H.}\\
    & = N \BR{b_1 d_1} + N\BR{\epsilon} + c
  \end{align*}

\item Case $\mathit{add}(\epsilon, b_2 d_2, c) = 
  \mathit{add}( \epsilon 0, b_2 d_2,c)$
  \begin{align*}
    N(\mathit{add}(\epsilon, b_2 d_2, c))
    &= N(\mathit{add}( \epsilon 0, b_2 d_2,c)) \\
    &= N\BR{\epsilon 0} + N\BR{b_2 d_2} + c \\
    &= N\BR{\epsilon} + N\BR{b_2 d_2} + c
  \end{align*}

\end{itemize}
\end{Answer}

\begin{Exercise}
\label{ex:mult}
Prove that $N(\mathit{mult}(b_1,b_2)) = N(b_1) N(b_2)$.
\end{Exercise}
\begin{Answer}
By induction on $\mathit{mult}$.
\begin{itemize}
\item Case $\mathit{mult}(b_1,\epsilon) = \epsilon$:\\
  \[
  N(\mathit{mult}(b_1,\epsilon)) = N(\epsilon) = 0 
  = N(b_1) N(\epsilon)
  \]

\item Case $\mathit{mult}(b_1, b'_2 0) = \mathit{mult}(b_1,b'_2) 0$:
  \begin{align*}
    N(\mathit{mult}(b_1, b'_2 0)) &= N(\mathit{mult}(b_1,b'_2) 0) \\
     & = 2 N(\mathit{mult}(b_1,b'_2)) \\
     & = 2 N(b_1) N(b'_2) & \text{by I.H.}\\
     & = N(b_1) N (b'_2 0)
  \end{align*}

\item Case $\mathit{mult}(b_1, b'_2 1) = 
  \mathit{add}(b_1, \mathit{mult}(b_1,b'_2)0)$:
  \begin{align*}
    N(\mathit{mult}(b_1, b'_2 1)) &= 
    N(\mathit{add}(b_1, \mathit{mult}(b_1,b'_2)0, 0)) \\
    &= N(b_1) + N(\mathit{mult}(b_1,b'_2)0) + 0 & \text{by Ex.~\ref{ex:add}}\\
    &= N(b_1) + 2 N(\mathit{mult}(b_1,b'_2)) \\
    &= N(b_1) + 2 N(b_1) N(b'_2) & \text{by I.H.}\\
    &= N(b_1) (2 N(b'_2) + 1) \\
    &= N(b_1) N(b'_2 1)
  \end{align*}
\end{itemize}
\end{Answer}

\begin{Exercise}
\label{binary-interp-correct}
 Prove that the interpreter is correct.
 That is
 \[
 N(I\BR{ e }) = E\BR{ e }
 \]
\end{Exercise}
\begin{Answer}
The proof is by induction on $e$.
\begin{itemize}
\item Case $e=n$: 
  $N(I\BR{ n }) = N\BR{ n } = E\BR{ n }$. 
\item Case $e = e_1 + e_2$: 
  \begin{align*}
    N(I\BR{e_1 + e_2}) &= N(\mathit{add}(I\BR{e_1}, I\BR{e_2}, 0)) \\
     & = N(I\BR{e_1}) + N(I\BR{e_2}) & \text{by Ex.~\ref{ex:add}}\\
     & = E\BR{e_1} + E\BR{e_2} & \text{by the I.H.}\\
     & = E\BR{e_1 + e_2}
  \end{align*}

\item Case $e = e_1 \times e_2$: 
  \begin{align*}
    N(I\BR{e_1 + e_2}) &= N(\mathit{mult}(I\BR{e_1}, I\BR{e_2})) \\
     & = N(I\BR{e_1}) \times N(I\BR{e_2}) & \text{by Ex.~\ref{ex:mult}}\\
     & = E\BR{e_1} \times E\BR{e_2} & \text{by the I.H.}\\
     & = E\BR{e_1 \times e_2}
  \end{align*}

\end{itemize}
\end{Answer}


%% \section{Booleans and Conditionals}

%% \[
%% \begin{array}{lrcl}
%% \text{expressions} & e & ::= & \ldots \mid \TRUE{} \mid \FALSE{} \mid \COND{e}{e}{e}
%% \end{array}
%% \]

%% local, immutable variables??

\section{Loops and Mutable Variables}
\label{sec:imp}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Schmidt:1986vn} Chapter 5 \\ 
Exercises: & 5.4, 5.5 a, 5.9
\end{tabular}
}

% IMP/WHILE
% arithmetic, booleans, mutable variables, 
% commands: assignment, sequence, if, while, skip, read, write.

Figure~\ref{fig:imperative} defines a language with mutable variables,
a variant of the
IMP~\citep{Plotkin:1983aa,Winskel:1993uq,Amadio:1998fk} and
WHILE~\citep{Hoare:1969kw} languages that often appear in textbooks on
programming languages. As a bonus, we include the \texttt{while} loop,
even though \citet{Schmidt:1986vn} does not cover loops until Chapter
6. The reason for his delayed treatment of \texttt{while} loops is
that their semantics is typically expressed in terms of fixed points
of continuous functions, which takes some time to explain. However, it
turns out that the semantics of \texttt{while} loops can be defined
more simply.

\subsection{Semantics}

To give meaning to mutable variables, we use a $\STORE$ which
is partial function from variables to numbers.
\[
  \STORE = \VAR \pto \FNAT
\]
We write $[x\mapsto n]s$ for removing the entry for $x$ is $s$
(if there is one) and then adding the entry $x\mapsto n$,
that is, $\{x\mapsto n\} \cup (s|_{\mathrm{dom}(s)-\{x\}})$.

\begin{figure*}[btp]
\begin{minipage}{0.5\textwidth}
\noindent Syntax
\[
\begin{array}{ll}
  \text{variables} & x \in \VAR  \\
 \text{expressions} & e \in \EXP ::= \ldots \mid x \\
 \text{conditions} & b \in \CND  ::= \TRUE \mid \FALSE \mid e = e \mid \\
  &   \qquad \neg b \mid b \lor b \mid b \land b \\
 \text{commands}& c \in \CMD ::= \SKIP \mid \ASSIGN{x}{e} \mid \SEQ{c}{c} \mid\\
  &      \qquad \IF{b}{c}{c} \mid \\
  &      \qquad \WHILE{b}{c} 
\end{array}
\]
\begin{gather*}
\intertext{$ \mathit{loop} : (\STORE {\to} \BOOL) {\to} \SET{\STORE{\times}\STORE} {\to} \SET{\STORE{\times}\STORE} $}
  \inference{m_b\,s = \FALSE}
            {(s,s) \in \mathit{loop}(m_b, m_c)}
\\[1ex]
  \inference{m_b\,s_1 = \TRUE & 
             (s_1,s_2) \in m_c \\
             (s_2, s_3) \in \mathit{loop}(m_b, m_c)}
            {(s_1, s_3) \in \mathit{loop}(m_b, m_c)}
\end{gather*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
Semantics 
\begin{align*}
\shortintertext{$P :  \CMD \to \SET{\FNAT \times \FNAT}$}
P\,c & = \{ (n,s'\,\mathtt{Z}) \mid (\{\mathtt{A}{\mapsto} n\},s') \in C\,c \}
\shortintertext{$ E :  \EXP \to \STORE \to \FNAT$}
  E\,x\,s & = s\,x \\
    & \vdots 
\shortintertext{$ B : \CND \to \STORE \to \BOOL$}
  B\,(e_1 = e_2)\,s &= (E\,e_1\,s) = (E\,e_2\,s) \\
  & \vdots 
\shortintertext{$C : \CMD \to \SET{\STORE \times \STORE}$} 
C\,\SKIP &= \mathit{id} \\ %\{(s,s) \mid s \in \STORE \}\\
C(\ASSIGN{x}{e}) &= \{ (s,[x\mapsto E\,e\,s]s) \mid s {\in} \STORE \}\\
C(\SEQ{c_1}{c_2}) &=  C\,c_2 \circ C\,c_1 \\
C\left(\!\!\begin{array}{l}
  \mathtt{if}\,b\,\mathtt{then}\,c_1\\
  \mathtt{else}\,c_2
  \end{array}\!\!\right)
&= \left\{ (s,s') \middle| \begin{array}{l}
  \mathit{if}\,B\,b\,s\,\mathit{then}\,(s,s') \in C\,c_1\\
  \mathit{else}\,(s,s')\in C\,c_2
\end{array}
\right\} \\
 %% \begin{cases}
 %%  C\,c_1 & \text{if } B\,b\,s = \TRUE \\
 %%  C\,c_2 & \text{if } B\,b\,s = \FALSE
 %% \end{cases} \\
C(\WHILE{b}{c}) &= \mathit{loop}(B\,b, C\,c)
\end{align*}
\end{minipage}
\caption{An Imperative Language: IMP}
\label{fig:imperative}
\end{figure*}

The syntax of expressions is extended to include variables, so the
meaning of expressions must be parameterized on the store. The meaning
of a variable $x$ is the associated number in the store $s$.
A program takes a number as input and it may produce a number or it
might diverge. Traditional denotational semantics model this behavior
with a partial function ($\FNAT\pto\FNAT$). Here we shall use the
alternate, but equivalent, approach of using a relation, a subset of
$\FNAT \times \FNAT$. A program is a command and the meaning of commands
is given by the semantic function $C$, which maps a command to a
relation on stores, that is, to a subset of $\STORE \times
\STORE$. The meaning of a program is given by the function
$P$, which initializes the store with the input number in variable
$\mathtt{A}$.  When the program completes, the output is obtained from
variable $\mathtt{Z}$.


We define the \texttt{while} loop using an auxiliary relation named
$\mathit{loop}$, which we define inductively in
Figure~\ref{fig:imperative}. Its two parameters are for the meaning of
the condition $b$ and the body $c$ of the loop.  If the meaning of the
condition $m_b$ is $\FALSE$, then the loop is already finished so the
starting and finishing stores are the same. If the condition $m_b$ is
$\TRUE$, then the loop executes the body, relating the starting store
$s_1$ to an intermediate store $s_2$, and then the loop continues,
relating $s_2$ to the finishing store $s_3$.

%% Function update:
%% \[
%%    [x\mapsto n]s(y) =
%%  \begin{cases} n & \text{if } y = x \\
%%    s(y) & \text{if } y \neq x
%%  \end{cases} 
%% \]

\subsection{Implementation}

We define an implementation of the imperative language, in terms of an
abstract machine, in Figure~\ref{fig:imp-impl}. The machine executes
one command at a time, similar to how a debugger such as \texttt{gdb}
can be used to view the execution of a C program one statement at a
time. Each command causes the machine to transition from one state to
the next, where a state is represented by a control component and the
store. The control component is the sequence of commands that need to
be executed, which is convenient to represent as a command of the
following form.
\[
\begin{array}{lrcl}
  \text{control} & k & ::= & \SKIP \mid \SEQ{c}{k}
\end{array}
\]
The partial function $\mathit{eval}$, also defined in
Figure~\ref{fig:imp-impl} in the main entry point for the abstract
machine.


\begin{figure*}

\hfill \fbox{$k, s \longrightarrow k', s'$}
\begin{align*}
  \SEQ{\SKIP}{k},\; s & \longrightarrow k,\; s \\
  \SEQ{(\ASSIGN{x}{e})}{k},\; s & \longrightarrow k,\; [x\mapsto E(e)(s)]s\\
  \SEQ{(\SEQ{c_1}{c_2})}{k},\; s & \longrightarrow
      \SEQ{c_1}{(\SEQ{c_2}{k})},\; s \\
  \SEQ{(\IF{b}{c_1}{c_2})}{k},\; s & \longrightarrow \SEQ{c_1}{k},\; s
     & \text{if } B\,b\,s = \TRUE \\
  \SEQ{(\IF{b}{c_1}{c_2})}{k},\; s & \longrightarrow \SEQ{c_2}{k},\; s
     & \text{if } B\,b\,s = \FALSE \\
  \SEQ{(\WHILE{b}{c})}{k},\; s & \longrightarrow
      \SEQ{c}{(\SEQ{(\WHILE{b}{c})}{k})},\; s
    & \text{if } B\,b\,s = \TRUE \\
  \SEQ{(\WHILE{b}{c})}{k},\; s & \longrightarrow k,\; s
    & \text{if } B\,b\,s = \FALSE 
\end{align*}
\[
  \mathit{eval}(c) = \{ (n,n') \mid
     (\SEQ{c}{\SKIP}), \{A\mapsto n\}
     \longrightarrow^{*} \SKIP, s'
     \text{ and } n' = s'(Z) \}
\]
\caption{Abstract Machine for IMP}
\label{fig:imp-impl}
\end{figure*}

Notation: given a relation $R$, we write $R(a)$ for the image of
$\{a\}$ under $R$. For example, if $R=\{ (0,4), (1,2), (1,3), (2,5)
\}$, then $R(1) = \{2,3\}$ and $R(0) = \{4\}$.

\begin{Exercise}
  Prove that if $k,s \longrightarrow k',s'$,
  then $C\,k\,s = C\,k'\,s'$.
\end{Exercise}
\begin{Answer}
The proof is by cases on $k,s \longrightarrow k',s'$.
\begin{itemize}
\item Case \fbox{$\SEQ{\SKIP}{k},\; s \longrightarrow k,\; s$}
  \[
  C(\SEQ{\SKIP}{k})(s) = (C(k) \circ C(\SKIP)) (s)
    = (C(k) \circ \mathit{id}) (s) = C(k)(s)
  \]

\item Case \fbox{$\SEQ{(\ASSIGN{x}{e})}{k},\; s \longrightarrow k,\; [x\mapsto E(e)(s)]s$}
\begin{align*}
 C(\SEQ{(\ASSIGN{x}{e})}{k})(s)
 &= 
 (C(k) \circ C(\ASSIGN{x}{e}))(s)\\
 &= 
 C(k)(C(\ASSIGN{x}{e})(s))\\
 &= 
 C(k)([x\mapsto E(e)(s)]s)
\end{align*}

\item Case \fbox{$\SEQ{(\SEQ{c_1}{c_2})}{k},\; s  \longrightarrow
      \SEQ{c_1}{(\SEQ{c_2}{k})},\; s$}
  \begin{align*}
    C(\SEQ{(\SEQ{c_1}{c_2})}{k})(s) 
    &=
    ( C(k) \circ (C(c_2) \circ C(c_1)) )(s) \\
    &= 
    ( (C(k) \circ C(c_2)) \circ C(c_1) )(s)\\
    &=
    C(\SEQ{c_1}{(\SEQ{c_2}{k})})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\IF{b}{c_1}{c_2})}{k},\; s 
        \longrightarrow \SEQ{c_1}{k},\; s$ and $B\,b\,s = \TRUE$}
  \begin{align*}
  C(\SEQ{(\IF{b}{c_1}{c_2})}{k})(s)
  &= (C(k) \circ C(\IF{b}{c_1}{c_2}))(s)\\
  &= (C(k) \circ C(c_1))(s)\\
  &= C(\SEQ{c_1}{k})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\IF{b}{c_1}{c_2})}{k},\; s 
        \longrightarrow \SEQ{c_2}{k},\; s$ and $B\,b\,s = \FALSE$}
  \begin{align*}
  C(\SEQ{(\IF{b}{c_1}{c_2})}{k})(s)
  &= (C(k) \circ C(\IF{b}{c_1}{c_2}))(s)\\
  &= (C(k) \circ C(c_2))(s)\\
  &= C(\SEQ{c_2}{k})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\WHILE{b}{c})}{k},\; s \longrightarrow
      \SEQ{c}{(\SEQ{(\WHILE{b}{c})}{k})},\; s$ and $B\,b\,s = \TRUE$}
  \begin{align*}
    C(\SEQ{(\WHILE{b}{c})}{k})(s)
    &= (C(k) \circ C(\WHILE{b}{c})) (s)\\
    &= (C(k) \circ (C(\WHILE{b}{c}) \circ C(c))) (s)\\
    &= ((C(k) \circ C(\WHILE{b}{c})) \circ C(c)) (s)\\
    &= C(\SEQ{c}{(\SEQ{(\WHILE{b}{c})}{k})})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\WHILE{b}{c})}{k},\; s \longrightarrow k,\; s$ 
  and $B\,b\,s = \FALSE$}
  \begin{align*}
    C(\SEQ{(\WHILE{b}{c})}{k})(s) 
    &= (C(k) \circ C(\WHILE{b}{c}))(s)\\
    &= (C(k) \circ \mathit{id})(s)\\
    &= C(k)(s)
  \end{align*}
\end{itemize}
\end{Answer}

\begin{Exercise}
  Prove that $P(c) = \mathit{eval}(c)$.
\end{Exercise}
\begin{Answer}
TODO
\end{Answer}

\begin{Exercise}
  Is the IMP language Turing-complete? Why or why not?
\end{Exercise}
\begin{Answer}
  The IMP defined here is not Turing-complete because the amount of
  memory is limited to a fixed amount because there are a fixed number
  of variables used in the program and each variable holds 64
  bits. Most versions of IMP are Turing complete because they
  ``cheat'': each variable contains a natural number, which has
  unbounded size.
\end{Answer}

\marginnote{
Is the semantic function $C$ sound wrt. contextual equivalence?
Is it complete? --Jeremy
}


\subsection{Semantics via Least Fixed Points}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Schmidt:1986vn} Chapter 6 \\
Exercises: & 6.2 a, 6.6, 8
\end{tabular}
}

We shall revisit the semantics of the imperative language, this time
taking the traditional but more complex approach of defining
\texttt{while} with a recursive equation and using least fixed points
to solve it. Recall that the meaning of a command is a partial
function from stores to stores, or more precisely,
\[
C\,c : \STORE \pto \STORE
\]
The meaning of a loop $(\WHILE{b}{c})$ is a solution to the
equation
\begin{equation}
\label{eq:w}
w = \LAM{s}\mathit{if}\, B\,b\,s \,\mathit{then}\, w(C\,c\,s)\,\mathit{else}\,s
\end{equation}
In general, one can write down recursive equations that do not have
solutions, so how do we know whether this one does?  When is there a
unique solution? The theory of least fixed points provides answers to
these questions.

\begin{definition}
A \textbf{\emph{fixed point}} of a function is an element that gets
mapped to itself, i.e., $x = f(x)$.
\end{definition}

In this case, the element that we're interested in is $w$, which is
itself a function, so our $f$ will be higher-order function. We
reformulate Equation~\ref{eq:w} as a fixed point equation by
abstracting away the recursion into a parameter $r$.
\begin{equation} \label{eq:while}
  w = F_{b,c}(w)
  \qquad
  \text{where }
  F_{b,c}\,r\,s = \mathit{if}\, B\,b\,s \,\mathit{then}\, r(C\,c\,s)\,\mathit{else}\,s
\end{equation}

There are quite a few theorems in the literature that guarantee the
existence of fixed points, but with different assumptions about the
function and its domain~\citep{Lassez:1982aa}. For our purposes, the
CPO Fixed-Point Theorem will do the job. The idea of this theorem is
to construct an infinite sequence that provides increasingly better
approximations of the least fixed point. The union of this sequence
will turn out to be the least fixed point.

The CPO Fixed-Point Theorem is quite general; it is stated in terms of
a function $F$ over partially ordered sets with a few mild conditions.
The ordering captures the notion of approximation, that is, we write
$x \sqsubseteq y$ if $x$ approximates $y$.

\begin{definition}
  A \textbf{\emph{partially ordered set (poset)}} is a pair
  $(L,\sqsubseteq)$ that consists of a set $L$ and a partial order
  $\sqsubseteq$ on $L$.
\end{definition}

For example, consider the poset
$(\mathbb{N}{\pto}\mathbb{N}, \subseteq)$ of partial
functions on natural numbers. Some partial function $f$ is a better
approximation than another partial function $g$ if it is defined on
more inputs, that is, if the graph of $f$ is a subset of the graph of
$g$.  Two partial functions are incomparable if neither is a subset of
the other.

\begin{marginfigure}
\centering\large
\xymatrix@=10pt{
 & \left\{
   \substack{2\mapsto 4, \\ 3\mapsto 9, \\4\mapsto 16}
  \right\} \ar@{-}[dl]\ar@{-}[d] \ar@{-}[dr]& \\
 \left\{ \substack{2\mapsto 4,\\ 3\mapsto 9} \right\} \ar@{-}[d]\ar@{-}[dr] 
  & 
 \left\{ \substack{2\mapsto 4,\\ 4\mapsto 16} \right\} \ar@{-}[dl]\ar@{-}[dr] 
  &
\left\{ \substack{3\mapsto 9,\\ 4 \mapsto 16} \right\} \ar@{-}[d]\ar@{-}[dl]
  \\
\{\substack{2\mapsto 4}\} \ar@{-}[dr] &  \{\substack{3\mapsto 9}\} \ar@{-}[d]& \{\substack{4\mapsto 16}\} \ar@{-}[dl]\\
  & \emptyset
}
\caption{A poset of partial functions.}
\label{fig:poset-of-partial-functions}
\end{marginfigure}
%
The sequence of approximations will start with the worst
approximation, a bottom element, written $\bot$, that contains no
information. (For example, $\emptyset$ is the $\bot$ for the poset of
partial functions.)
%% \begin{wrapfigure}{r}{0.33\textwidth}
%% \small \textbf{Example} For \texttt{while}, each $g_i$ corresponds to
%% iterating the loop up to $i$ times.
%% \end{wrapfigure}
The sequence proceeds to by applying $F$ over and over again, that is,
\[
  \bot \quad F(\bot)\quad F(F(\bot))\quad F(F(F(\bot)))\quad \cdots\quad F^i(\bot)\quad \cdots
\]

But how do we know that this sequence will produce increasingly better
approximations?  How do we know that
\[
F^i(\bot) \sqsubseteq F^{i+1}(\bot)
\]
We can ensure this by requiring the output of $F$ to improve when the
input improves, that is, require $F$ to be monotonic.

\begin{definition}
  Given two partial orders $(A,\sqsubseteq)$ and
  $(B,\sqsubseteq)$, $F : A {\to} B$ is \textbf{\emph{monotonic}} iff
  for any $x,y\in A$, $x \sqsubseteq y$ implies $F(x) \sqsubseteq F(x)$.
\end{definition}

\begin{proposition}
  The functional $F_{b,c}$ for \texttt{while} loops \eqref{eq:while}
  is monotonic.
\end{proposition}
\begin{proof}
  Let $f, g : \STORE \pto \STORE$ such that
  $f \subseteq g$. We need to show that $F_{b,c}(f) \subseteq
  F_{b,c}(g)$. 
  Let $s$ be an arbitrary state. Suppose $B\,b\,s = \TRUE$.
  \[
  F_{b,c}\,f\,s = f(C\,c\,s) \subseteq g(C\,c\,s) = F_{b,c}\,g\,s 
  \]
  So we have $F_{b,c}(f) \subseteq F_{b,c}(g)$.
  Next suppose $B\,b\,s = \FALSE$.
  \[
  F_{b,c}\,f\,s = s
               = F_{b,c}\,g\,s 
  \]
  So again we have $F_{b,c}(f) \subseteq F_{b,c}(g)$.  Having
  completed both cases, we conclude that $F_{b,c}(f) \subseteq
  F_{b,c}(g)$.
\end{proof}


We have $\bot \sqsubseteq F(\bot)$ because $\bot$ is less or equal to
everything. Then we apply monotonicity to obtain $F(\bot) \sqsubseteq
F(F(\bot))$.  Continuing in this way we obtain the sequence of
increasingly better approximations in Figure~\ref{fig:kleene-chain}.
If at some point the approximation stops improving and $F$ produces an
element that is equal to the last one, then we have found a fixed
point. However, because we are interested in elements that are partial
functions, which are infinite, the sequences of approximations will
also be infinite. So we'll need some other way to go from the
sequences of approximations to the actual fixed point.

\begin{marginfigure}
\[
  \xymatrix@=10pt{
    \ar@{..}^{\sqsubseteq}[d]\\
    F^i(\bot) \ar@{..}^{\sqsubseteq}[d]\\
    F(F(F(\bot))) \ar@{-}^{\sqsubseteq}[d]\\
    F(F(\bot)) \ar@{-}^{\sqsubseteq}[d]\\
    F(\bot) \ar@{-}^{\sqsubseteq}[d]\\
    \bot
  }
\]
%% \[
%%   \bot \sqsubseteq F(\bot) \sqsubseteq F^2(\bot) 
%%     \sqsubseteq F^3(\bot) \sqsubseteq \cdots
%% \]
\caption{Ascending sequence of $F$.}
\label{fig:kleene-chain}
\end{marginfigure}

The solution is to take the union of all the approximations. The
analogue of union for an arbitrary partial order is least upper bound.

\begin{marginfigure}
$\bigsqcup \left\{\!\! \begin{array}{l}
              \{ 2 \mapsto 4, 3 \mapsto 9 \}, \\
              \{ 3 \mapsto 9, 4 \mapsto 16 \} 
                 \end{array}\!\! \right\}
= \left\{\!\! \begin{array}{l}
        2 \mapsto 4, \\ 3 \mapsto 9, \\ 4 \mapsto 16 
    \end{array}\!\! \right\}$
\caption{The lub of partial functions.}
\end{marginfigure}

\begin{definition}
Given a subset $S$ of a partial order $(L,\sqsubseteq)$, an
\textbf{\emph{upper bound}} of $S$ is an element $y$ such that for all
$x \in S$ we have $x \sqsubseteq y$.  The \textbf{\emph{least upper
    bound (lub)}} of $S$, written $\bigsqcup S$, is the least of all
the upper bounds of $S$, that is, given any upper bound $z$ of $S$, we
have $\bigsqcup S \sqsubseteq z$.
\end{definition}

In arbitrary posets, a least upper bound may not exist for a
subset. In particular, for the poset $(\mathbb{N}{\pto}\mathbb{N},
\subseteq)$, two partial functions do not have a lub if they are
inconsistent, that is, if they map the same input to different
outputs, such as $\{3\mapsto 8\}$ and $\{3 \mapsto 9\}$. However, the
CPO Fixed-Point Theorem will only need to consider totally ordered
sequences, and all the elements in such a sequence are consistent.

\begin{definition}
  A \textbf{\emph{complete partial order (cpo)}} has a least upper
  bound for every sequence of totally ordered elements.
\end{definition}


%% \begin{proof}
%%   Let $S$ be a directed subset of $A \to B_\bot$.
%%   The union of the elements of $S$ is the least upper bound of $S$.
%%   Suppose $h \in S$. Then $h \subseteq \bigcup S$, so $\bigcup S$ is an upper bound of $S$.
%%   Let $f$ be an arbitrary upper bound of $S$.
%%   We need to show that $\bigcup S \subseteq f$.
%%   Suppose $(x,y) \in \bigcup S$.
%%   Then $(x,y) \in g$ for some $g \in S$.
%%   So $(x,y) \in f$ because $g \subseteq f$.
%%   Therefore $\botcup S$ is the least of all the upper bounds of $S$.
%% \end{proof}

%% I like dcpos because it seems quite natural to say that lubs should
%% exist for sets of consistent elements. On the other hand, the
%% chain-based definition gives just what is needed for Kleene's Fixed
%% Point Theorem.
%% The two schools of thought yield definitions that are
%% equivalent~\citep{Mitchell:1996nn} (page 312).

\begin{proposition}
  The poset of partial functions $(A\pto B,\subseteq)$ is a
  cpo.
\end{proposition}
\begin{proof}
  Let $S$ be a totally ordered sequence in $(A\pto B,\subseteq)$.  We
  claim that the lub of $S$ is the union of all the elements of
  $S$, that is $\bigcup S$. Recall that $\forall x y, (x,y) \in
  \bigcup S$ iff $\exists f \in S, (x,y) \in f$.
%
  We first need to show that $\bigcup S$ is an upper bound of $S$.
  Suppose $f \in S$. We need to show that $f \subseteq \bigcup S$.
  Consider $(x,y) \in f$. Then $(x,y) \in \bigcup S$. So indeed,
  $\bigcup S$ is an upper bound of $S$.  Second, consider another
  upper bound $g$ of $S$.  We need to show that $\bigcup S \subseteq
  g$. Suppose $(x,y) \in \bigcup S$.  Then $(x,y) \in h$ for some $h
  \in S$. Because $g$ is an upper bounf of $S$, we have $h \subseteq
  g$ and therefore $(x,y) \in g$. So we conclude that $\bigcup S
  \subseteq g$.
\end{proof}

The last ingredient required in the proof of the fixed point theorem
is that the output of $F$ should only depend on a finite amount of
information from the input, that is, it should be continuous. For
example, if the input to $F$ is itself a function $g$, $F$ should only
need to apply $g$ to a finite number of different values.  This
requirement is at the core of what it means for a function to be
computable~\citep{Gunter:1990aa}.  So applying $F$ to the lub of a
sequence $S$ should be the same as taking the lub of the set obtained
by mapping $F$ over each element of $S$.

\begin{definition}
  A monotonic function $F : A {\to} B$ on a cpo is
  \textbf{\emph{continuous}} iff for all totally ordered sequences $S$
  of $A$
  \[
  F(\bigsqcup S) = \bigsqcup \{ F(x) \mid x \in S \}.
  \]
\end{definition}

\begin{proposition}
  The functional $F_{b,c}$ for \texttt{while} loops \eqref{eq:while}
  is continuous.
\end{proposition}
\begin{proof}
  Let $S$ be a totally ordered sequence in $(A\pto B,\subseteq)$.  We
  need to show that
  \[
  F_{b,c}(\bigcup S) = \bigcup \{ F_{b,c}(f) \mid f \in S\}
  \]
  We shall prove this equality by showing that each graph is a subset
  of the other.  We assume $(s,s'') \in F_{b,c}\,(\bigcup S)$.
  Suppose $B\,b\,s = \TRUE$.  Then $(s,s'') \in (\bigcup S) \circ (C
  \, c)$, so $(s,s') \in C\,c$ and $(s',s'') \in g$ for some $s'$ and
  $g \in S$. So $(s,s'') \in g \circ (C\, c)$.  Therefore $(s,s'') \in
  \bigcup \{ F_{b,c}(f) \mid f \in S \}$.  So we have shown that
  $F_{b,c}\,(\bigcup S) \subseteq \bigcup \{ F_{b,c}(f) \mid f \in S
  \}$. Next suppose $B\,b\,s = \FALSE$.  Then
  $
  F_{b,c}(\bigcup S) = \mathit{id} = \bigcup \{ F_{b,c}(f) \mid f \in S\}
  $.

  For the other direction, we assume $(s,s'') \in \bigcup \{
  F_{b,c}(f) \mid f \in S\}$. So $(s,s'') \in F_{b,c}(g)$ for some $g
  \in S$.  Suppose $B\,b\,s = \TRUE$.  Then $(s,s') \in C\,c$ and
  $(s',s'') \in g$ for some $s'$. So $(s',s'') \in \bigcup S$ and
  therefore $(s,s'') \in F_{b,c}(\bigcup S)$.  So we have shown that
  $\bigcup \{ F_{b,c}(f) \mid f \in S \} \subseteq F_{b,c}\,(\bigcup
  S)$. Next suppose $B\,b\,s = \FALSE$. Then again we have both graphs
  equal to the identity relation.
\end{proof}

We now state the fixed point theorem for cpos.

\begin{theorem}[CPO Fixed-Point Theorem]\label{thm:fixed-point}
Suppose $(L,\sqsubseteq)$ is a cpo and let $F:L\to L$ be a continuous
function. Then $F$ has a least fixed point, written $\mathrm{fix}\,F$,
which is the least upper bound of the ascending sequence of $F$:
\[
  \mathrm{fix}\,F = \bigsqcup \{ F^n(\bot) \mid n \in \NAT \}
\]
\end{theorem}
\begin{proof}
Note that $\bot$ is an element of $L$ because it is the lub of the
empty sequence.  We first prove that $\mathrm{fix}\,F$ is a fixed
point of $F$.
\begin{align*}
  F(\mathrm{fix}\,F) &= F(\bigsqcup \{ F^n(\bot) \mid n \in \NAT \})\\
  &= \bigsqcup\{ F(F^n(\bot)) \mid n \in \NAT \} & \text{by continuity}\\
  &= \bigsqcup\{ F^{n+1}(\bot)) \mid n \in \NAT \} \\
  %% &= \bigsqcup \{ F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  %% &= \bigsqcup \{ F^0(\bot), F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  &= \bigsqcup\{ F^n(\bot)) \mid n \in \NAT \} 
  & \text{because } F^0(\bot) = \bot \sqsubseteq F^1(\bot)  \\
  &= \mathrm{fix}\,F
\end{align*}
Next we prove that $\mathrm{fix}\,F$ is the least of the fixed points
of $F$. Suppose $e$ is an arbitrary fixed point of $F$. By the
monotonicity of $F$ we have $F^i(\bot) \sqsubseteq F^i(e)$ for all
$i$.  And because $e$ is a fixed point, we also have $F^i(e) = e$, so
$e$ is an upper bound of the ascending chain, and therefore
$\mathrm{fix}\,F \sqsubseteq e$.
\end{proof}


Returning to the semantics of the \texttt{while} loop, we give the
least fixed-point semantics of an imperative language in
Figure~\ref{fig:imperative-fixed-point}. We have already seen that the
poset of partitial functions is a cpo and that $F_{b,c}$ is
continuous, so $\mathrm{fix}\,(F_{b,c})$ is well defined and is the
least fixed-point of $F_{b,c}$. Thus, we can define the meaning
of the \texttt{while} loop as follows.
\[
  C(\WHILE{b}{c})\,s = \mathrm{fix}(F_{b,c})\,s
\]

\begin{Exercise}
  Prove that the semantics in Figure~\ref{fig:imperative} is
  equivalent to the least fixed-point semantics in
  Figure~\ref{fig:imperative-fixed-point}, i.e., $(n,n') \in P\,c$ iff
  $P\,c\,n = n'$.
\end{Exercise}
\begin{Answer}
TODO
\end{Answer}

\begin{figure}[tbp]
\begin{align*}
  \intertext{$P' : \CMD \to \NAT \pto \NAT$}
  P'\,c\,n &= (C'\,c\,\{A{\mapsto} n]\})\,Z 
\end{align*}
\begin{align*}
  \intertext{$C' : \CMD \to \STORE \pto \STORE$}
C'\,\SKIP\,s &= s \\ 
C'(\ASSIGN{x}{e})\,s &= [x\mapsto E\,e\,s]s\\
C'(\SEQ{c_1}{c_2})\,s &=  C'\,c_2(C'\,c_1\,s) \\
C'\left(\!\!\begin{array}{l}
  \mathtt{if}\,b\,\mathtt{then}\,c_1\\
  \mathtt{else}\,c_2
  \end{array}\!\!\right)\,s
  &= 
 \begin{cases}
  C'\,c_1\,s & \text{if } B\,b\,s = \TRUE \\
  C'\,c_2\,s & \text{if } B\,b\,s = \FALSE
 \end{cases} \\
C'(\WHILE{b}{c})\,s &= \mathrm{fix}(F_{b,c})\,s
\end{align*}
\caption{Least Fixed-Point Semantics of an Imperative Language}
\label{fig:imperative-fixed-point}
\end{figure}

In the literature there are two schools of thought regarding how to
define complete partial orders. There is the one presented above, that
requires lubs to exists for all totally ordered
sequences~\citep{Plotkin:1983aa,Schmidt:1986vn,Winskel:1993uq}.  The
other requires that the poset be directed complete, that is, lubs
exists for all directed
sets~\citep{Gunter:1990aa,Mitchell:1996nn,Amadio:1998fk}.  The two
schools of thought are equivalent, i.e., a poset $P$ with a least
element is directed-complete iff every totally ordered sequence in $P$
has a lub~\citep{Davey:2002fj} (Theorem 8.11).


\section{Simply-Typed Functions}
\label{sec:stlc}

\marginnote{
\begin{tabular}{ll}
  Reading: & \citet{Gunter:1992aa} Chapter 2, \\
    & \citet{Chlipala:2007fk}
\end{tabular}
}

The appropriately-named simply-typed $\lambda$-calculus (STLC)
provides a simple setting in which to study first-class functions.
That is, functions that can be used like any other data.  In the
limited context of the STLC, this just means that they can be passed
as parameters and returned from functions.  Figure~\ref{fig:stlc}
defines the syntax and type system of the STLC. Whenever we discuss
expression of the STLC, we always assume that they are well-typed,
that is, that they satisfy the type system defined in
Figure~\ref{fig:stlc}.  The types include natural numbers and
functions $A \to B$ where $A$ is the type of the input (there is just
one) and $B$ is the type of the output.

The purpose of the variables in the STLC is for refering to the
parameters of functions. The expression $\LAM{x\of A}e$ creates a
function of one parameter named $x$ of type $A$. The expression $e$ is
the body of the function and it may refer to parameter $x$.  The value
of $e$ is the return value of the function. The expression $e_1 \APP
e_2$ \emph{applies} the function produced by $e_1$ to the value of
$e_2$.  The type system ensures that we never apply numbers (as if
they were functions) or perform arithmetic on functions (as if they
were numbers). The type system also prevents the use of undefined
variables.


\begin{figure}[tbp]
\noindent Syntax
\[
\begin{array}{lrl}
 \text{types} & A,B ::= & \NATTY \mid A \to B \\
 \text{numbers}& n \in & \mathbb{N}  \\
  \text{variables}& x \in & \VAR  \\
 \text{arithmetic} & \oplus ::= & + \mid \times \\
 \text{expressions}& e ::=& n \mid e \oplus e
   \mid x \mid \LAM{x\of A} e \mid e \APP e \\
 \text{type env.} & \Gamma ::= & \emptyset \mid \Gamma,x\of A
\end{array}
\]
Type System
\begin{gather*}
  \inference{}{\Gamma \vdash n : \NATTY}
  \quad
  \inference{\Gamma \vdash e_1 : \NATTY & \Gamma \vdash e_2 : \NATTY}
            {\Gamma \vdash e_1 \oplus e_2 : \NATTY}
  \\[1ex]
  \inference{\Gamma_n = x\of A}{\Gamma \vdash x : A}
  \quad
  \inference{\Gamma,x\of A \vdash e : B}
            {\Gamma \vdash \LAM{x\of A}e : A \to B}
  \quad
  \inference{\Gamma \vdash e_1 : A \to B \\ \Gamma \vdash e_2 : A}
            {\Gamma \vdash e_1 \APP e_2 : B}
\end{gather*}
\caption{Syntax and types of the simply-typed $\lambda$-calculus}
\label{fig:stlc}
\end{figure}

\subsection{Semantics}

The denotational semantics of the STLC is simple because STLC
functions are total, they always terminate with an answer. So they can
be straightforwardly modeled by mathematical (total) functions. But
there is one twist, describing the type of the semantic function is a
bit interesting.  The set of values that can be produced by an STLC
expression \emph{depends} on the type of the expression. For example,
if an expression has type $\NATTY$, then its meaning will be in
$\NAT$. If an expression has type $\NATTY \to \NATTY$, then its
meaning will be in $\NAT^{\NAT}$
\footnote{The set-theoretic notation $B^A$ refers to the set of all
  total functions from set $A$ to set $B$.}.  Thus, we define a
mapping $E$ from each type into a set.
\begin{align*}
  E\,\NATTY &= \mathbb{N} \\
  E\,(A \to B) &=  (E\,B)^{E\,A}
\end{align*}
Likewise, we define a mapping from type environments to sets, in
particular, to a cartesian product of the variable bindings.
\begin{align*}
  E\,\emptyset &= \UNITTY \\
  E\,(\Gamma,x\of A) &= (\VAR \times E\,A) \times E\,\Gamma
\end{align*}

With these mapping in hand, we can describe the semantic function $E$
for the STLC. Because of the dependency described above, the type of
$E$ is interesting. It is a dependent function whose first parameter
is some well-typed expression $e$, so we have $\Gamma \vdash e : A$.
The second parameter is a type environment drawn from $E\,\Gamma$ and
the result is in $E\,A$.  The definition of the semantic function $E$
is given in Figure~\ref{fig:sem-stlc}.  We interpret each $\lambda$ as
a function that maps every element $d$ in its domain $E\,A$ to the
element produced by the body $e$ of the $\lambda$, with $d$ bound to
parameter $x$. We interpret application as mathematical function
application. Regarding variables, their meaning is given by the
environment $\rho$.

\begin{figure}[tbp]
\begin{align*}
\intertext{$E : \prod (e : \EXP), E\,\Gamma \to E\,A \qquad \text{where } \Gamma \vdash e : A$}
  E\,n\,\rho &= n \\
  E\,(e_1\oplus e_2)\,\rho &= \delta(\oplus,n_1, n_2) \\
     & \text{if } n_1 {=} E\,e_1\,\rho \text{ and } n_2 {=} E\,e_2\,\rho \\
  E\,x\,\rho &= \rho(x) \\
  E\,(\LAM{x\of A}e)\,\rho &= \{ (d,d') \mid d \in E\,A \text{ and } d' = E\,e\,\ext{x}{d}{\rho} \} \\
  E\,(e_1 \APP e_2)\,\rho &= (E\,e_1\,\rho) \APP (E\,e_2\,\rho)
\end{align*}

\caption{Semantics of the simply-typed $\lambda$-calculus}
\label{fig:sem-stlc}
\end{figure}


\subsection{Equational Theory}

Another way to specify the meaning of a language is with an equational
theory. Figure~\ref{fig:stlc-eqns} gives the standard equational rules
for the STLC, and the equational theory is the set of all equalities
that can be deduced from these rules.  Note that these rules are
closely related to the reduction rules of the STLC; they basically
specify a bidirectional version of those rules.

\begin{figure}[tbp]
\begin{gather*}
  \text{(refl)}
  \inference{}{\vdash e = e}
  \quad
  \text{(sym)}
  \inference{\vdash e_2 = e_1}{\vdash e_1 = e_2}
  \quad
  \text{(trans)}
  \inference{\vdash e_1 = e_2 & \vdash e_2 = e_3}{\vdash e_1 = e_3}  
  \\[2ex]
  \text{(cong)}
  \inference{\vdash e_1 = e_3 & \vdash e_2 = e_4}
    {\vdash (e_1\APP e_2) = (e_3 \APP e_4)}
  \quad
  (\xi)\inference{\vdash e = e'}{\vdash \LAM{x}e = \LAM{x}e'}
  \\[2ex]
  (\beta) \inference{}{\vdash (\LAM{x\of A}e)\APP e' = [x\by e']e}
  \quad
  (\eta) \inference{x \notin \mathrm{FV}(e)}
      {\vdash (\LAM{x\of A} e \APP x) = e}
  \\[2ex]
  \text{(cong-$\oplus$)}
  \inference{\vdash e_1 = e_3 & \vdash e_2 = e_4}
    {\vdash (e_1\oplus e_2) = (e_3 \oplus e_4)}
  \quad
  \text{($\delta$)}\inference{n_3 = \delta(\oplus,n_1,n_2)}{n_1\oplus n_2 = n_3}
\end{gather*}
\caption{Equational Theory of the simply-typed $\lambda$-calculus}
\label{fig:stlc-eqns}
\end{figure}


\subsection{Soundness of Equational Theory}

Next we check whether the equational theory is sound with respect to
the model. That is, for each equation that holds in the theory, does
it also hold in the model?

The most intersting of the equations is $\beta$, which involves
substitution. So we shall need the following lemma regarding the
interaction between substitution and the semantic function $E$.

\begin{lemma}[Substitution]
  \label{lem:stlc-subst}
  Suppose $\Gamma,x \of B \vdash e : A$ and $\Gamma \vdash e' : B$.
  $E\,([x\by e']e)\,\rho = E\,e\,\ext{x}{E\,e'\,\rho}{\rho}$
\end{lemma}
\begin{proof}
  The proof is by induction on $e$.
  \begin{itemize}
  \item Case $e=n$:
    \[
    E\,([x\by e']n)\,\rho = n = E\,n\,\ext{x}{E\,e'\,\rho}{\rho}
    \]
  \item Case $e=e_1 \oplus e_2$:
    \begin{align*}
      E\,([x\by e'](e_1 \oplus e_2))\,\rho 
      &= E\,([x\by e']e_1 \oplus [x\by e']e_2))\,\rho  & \text{def. substitution}\\
      &= E\,([x\by e']e_1)\,\rho \oplus E\,([x\by e']e_2)\,\rho  & \text{def. $E$ } \\
      &= E\,e_1\,\ext{x}{E\,e'\,\rho}{\rho} \oplus E\,e_2\,\ext{x}{E\,e'\,\rho}{\rho} & \text{I.H.}\\
      &= E\,(e_1 \oplus e_2)\,\ext{x}{E\,e'\,\rho}{\rho} & \text{def. $E$}
    \end{align*}

  \item Case $e=y$:
    We consider two cases, whether $y=x$ or not.
    \begin{itemize}
    \item Subcase $y=x$:
      \[
        E\,([x\by e']x)\,\rho 
        = E\,e'\,\rho 
        = E\,x\,\ext{x}{E\,e'\,\rho}{\rho}
        \]
    \item Subcase $y\neq x$:
      \[
      E\,([x\by e']y)\,\rho
      =
      E\,y\,\rho
      = E\,y\,\ext{x}{E\,e'\,\rho}{\rho}
      \]
    \end{itemize}

  \item Case $e=\LAM{y \of A} e_1$: \\
    By $\alpha$-conversion we can make sure that $y \notin
    \mathrm{fv}(e')$ and $y \neq x$.
    \begin{align*}
      E\,[x\by e'](\LAM{y \of A} e_1)\,\rho
      &= E\,(\LAM{y \of A} [x\by e']e_1)\,\rho &\text{by def. substitution} \\
      &= \{ (d,d') \mid d \in E\,A \text{ and }
          d' {=} E \,([x\by e']e_1)\,\ext{y}{d}{\rho} \} & \text{by def. $E$} \\
      &= \{ (d,d') \mid d \in E\,A \text{ and }
          d' {=} E \,e_1\,\ext{x}{E\,e'\,\ext{y}{d}{\rho}}{\ext{y}{d}{\rho}} \} & \text{by I.H.} \\
      &= \{ (d,d') \mid d \in E\,A \text{ and }
          d' {=} E \,e_1\,\ext{x}{E\,e'\,\rho}{\ext{y}{d}{\rho}} \} & y \notin \mathrm{FV}(e') \\
      &= \{ (d,d') \mid d \in E\,A \text{ and }
          d' {=} E \,e_1\,\ext{y}{d}{\ext{x}{E\,e'\,\rho}{\rho}} \} & y \neq x \\
      &= E\,(\LAM{y \of A} e_1)\,\ext{x}{E\,e'\,\rho}{\rho} & \text{by def. $E$}
    \end{align*}
    
  \item Case $e=e_1 \APP e_2$:
    \begin{align*}
      E\,[x\by e'](e_1 \APP e_2)\,\rho
      &= E\,([x\by e']e_1 \APP [x\by e']e_2)\,\rho & \text{by def. substitution}\\
      &= (E\,[x\by e']e_1\,\rho) \APP (E\,[x\by e']e_2\,\rho) & \text{by def. $E$} \\
      &= (E\,e_1\,\ext{x}{E\,e'\,\rho}{\rho}) \APP
      (E\,e_2\,\ext{x}{E\,e'\,\rho}{\rho}) & \text{by I.H.} \\
      &= E\,(e_1 \APP e_2)\,\ext{x}{E\,e'\,\rho}{\rho} & \text{by def. $E$}
    \end{align*}
  \end{itemize}
\end{proof}


\begin{definition}
Suppose $M$ is a semantic function for the STLC.  A
$\Gamma$-environment $\rho$ maps each variable $x : A \in \Gamma$ to
an element of the set $M\,A$, i.e. to an equivalence class of
expressions of type $A$.
\[
  \rho \models \Gamma \quad\defeq\quad \forall x:A \in \Gamma, \rho(x) \in M\,A
\]
\end{definition}

\begin{definition}
  Suppose $\Gamma \vdash e : A$, $\Gamma \vdash e' : A$, and $M$ is a
  semantic function for STLC.
  \[
  M \models e = e' \quad \defeq \quad \forall \rho,\;
  \rho \models \Gamma \implies M\,e\,\rho = M\,e'\,\rho
  \]
\end{definition}

\begin{theorem}[Soundness of Theory wrt. the Model]\ \\
  \noindent
  If $\vdash e = e'$, then $E \models e = e'$.
\end{theorem}
\begin{proof}
  Let $\rho$ be an environment such that $\rho \models \Gamma$.
  We need to show that $E\,e\,\rho = E\,e'\,\rho$.
  The proof is by induction on the derivation of $\vdash e_1 = e_2$.
  \begin{itemize}
  \item (refl), (sym), (trans) are immediate.
  \item (cong)
    Let $\rho$ be an arbitrary environment.
    By the induction hypothesis, we have $E\,e_1\,\rho = E\,e_3\,\rho$
    and $E\,e_2\,\rho = E\,e_4\,\rho$. Therefore
    $(E\,e_1\,\rho)\APP(E\,e_2\,\rho) = (E\,e_3\,\rho) \APP (E\,e_4\,\rho)$.
    
  \item ($\xi$) Let $\rho$ be an arbitrary environment.  We need to
    show that $E\,(\LAM{x}e)\,\rho = E\,(\LAM{x}e')\,\rho$. Thus, it
    suffices to show for an arbitrary $d$ that
    $E\, e\, \ext{x}{d}{\rho} = E\, e'\, \ext{x}{d}{\rho}$,
    but that follows from the induction hypothesis.
    
  \item ($\beta$) Let $\rho$ be an arbitrary environment.
    \begin{align*}
      E\,((\LAM{x\of A}e)\APP e')\,\rho &=
      (E\,(\LAM{x\of A}e)\,\rho) (E\,e'\,\rho) \\
      &= E\,e\,\ext{x}{E\,e'\,\rho}{\rho} \\
      &= E\,[x\by e']e\,\rho  & \text{by Lemma~\ref{lem:stlc-subst}}
    \end{align*}
    
  \item ($\eta$) Let $\rho$ be an arbitrary environment.
    \begin{align*}
      E\,(\LAM{x} e \APP x)\,\rho &=
      \uplambda d.\,E\,(e \APP x)\,\ext{x}{d}{\rho} \\
      &= \uplambda d.\,(E\,e\,\ext{x}{d}{\rho}) \APP d \\
      &= \uplambda d.\,(E\,e\,\rho) \APP d & \text{because } x \notin \mathrm{FV}(e)\\
      &= E\,e\,\rho
    \end{align*}

  \item (cong-$\oplus$) TODO

  \item ($\delta$) TODO

  \end{itemize}
\end{proof}


Next we turn to the question of completeness of the equational theory
wrt. to the model. This is more involved and first requires that we
construct the so-called term model.

\subsection{Term Model}

In this section we construct a model for STLC that is built from sets
of expressions of STLC modulo the equational theory. We then use this
model in the next section in the proof of completeness.

We define the equivalence classes of expressions that are equal
according to the theory.
\[
  [e] \defeq \{ e' \mid \; \vdash e = e' \}
\]
We also define the equivalence classes of a given type.  Fix a type
environment $\mathcal{G}$ with an infinite number of distinct variable
bindings for every type.
\[
T\,A \defeq \{ [e] \mid \exists \Gamma, \Gamma \subset \mathcal{G}
  \text{ and } \Gamma \vdash e : A \}
\]
Given $[e_1] \in T\,(A\to B)$ and $[e_2] \in T\,A$, we define an
application operator as follows:
\[
    [e_1] \cdot [e_2] \defeq [(e_1\APP e_2)]
\]

\begin{definition}
A structure $(M,\cdot)$ is a \emph{pre-frame} if extensionality
holds, i.e. if two functions $f,g \in M (A \to B)$ map every argument
$a \in M\,A$ to equal results, $f \cdot a = g \cdot a$, then the two
functions are the same function, $f = g$.
\end{definition}

\begin{lemma}
  The term model $(T,\cdot)$ is a pre-frame.
\end{lemma}
\begin{proof}
Let $[f],[g] \in T\,(A\to B)$.  We assume $[f] \cdot [a] = [g] \cdot
[a]$ for any $[a]\in T\,A$.  We need to show that $[f] = [g]$, so it
suffices to show $\vdash f = g$.  Find some $x : A \in \mathcal{G}$
s.t. $x \notin \mathrm{FV}(f) \cup \mathrm{FV}(g)$.  Then
\[
  [f \APP x] = [f] \cdot [x] = [g] \cdot [x] = [g \APP x]
\]
and so $\vdash f \APP x = g \APP x$. Then by the ($\xi$) rule we have
\[
\vdash \LAM{x\of A} f \APP x = \LAM{x\of A} g \APP x
\]
By the ($\eta$) rule we have $\vdash \LAM{x\of A} f \APP x = f$ and
$\vdash \LAM{x\of A} g \APP x = g$, so we conclude that $\vdash f = g$.
\end{proof}

\begin{definition}
A structure $(M,\cdot)$ is a \emph{frame} if $(M,\cdot)$ is a
pre-frame and $M$ is also a mapping on expressions that satisfies the
following equations
\begin{align*}
  M\,n\,\rho &= n \\
  M\,(e_1 \oplus e_2)\,\rho &= M\,e_1\,\rho \oplus M\,e_2\,\rho \\
  M\,x\,\rho &= \rho(x) \\
  M\,(e_1\APP e_2)\,\rho &= (M\,e_1\,\rho) \cdot (M\,e_2\,\rho) \\
  (M\,(\LAM{x\of A} e_1)\,\rho) \cdot d &=
    M\,e_1\,\ext{x}{d}{\rho}
\end{align*}
\end{definition}

In the term model, a $\Gamma$-environment $\rho$ maps each variable $x
: A \in \Gamma$ to an element of the set $T\,A$, i.e. to an
equivalence class of expressions of type $A$.
\[
  \rho \models \Gamma \defeq \forall x:A \in \Gamma, \rho(x) \in T\,A
\]
A substitution $\sigma$ maps each variable to a single expression. A
substitution $\mathsf{rep}(\rho)$ represents $\rho$ if it maps each
variable to the representative in $\rho$.
\[
  \mathsf{rep}(\rho)(x) \defeq e \quad \text{if } \rho(x) = [e]
\]

The meaning of expressions in the term model is defined as follows.
\[
  T\,e\,\rho \defeq [\mathsf{rep}(\rho)(e)]
\]


\begin{lemma}
  \label{lem:term-frame}
  The term model $(T,\cdot)$ is a frame.
\end{lemma}
\begin{proof} Let $\sigma = \mathsf{rep}(\rho)$.
  \begin{itemize}
  \item nts. $T\,x\,\rho = \rho(x)$.
    \[
     T\,x\,\rho = [\sigma(x)] = \rho(x)
    \]
    
  \item nts. $T\,(e_1\APP e_2)\,\rho = (T\,e_1\,\rho) \cdot (T\,e_2\,\rho)$
    \[
    T\,(e_1\APP e_2)\,\rho
    = [\sigma(e_1\APP e_2)]
    = [\sigma(e_1)] \cdot [\sigma(e_2)]
    = (T\,e_1\,\rho ) \cdot (T\,e_2\,\rho)
    \]

  \item nts. $(T\,(\LAM{x\of A} e_1)\,\rho) \cdot d =
    T\,e_1\,\ext{x}{d}{\rho}$. Let $d=[e]$.
    \begin{align*}
    (T\,(\LAM{x\of A} e_1)\,\rho) \cdot [e]
    &= [\LAM{x\of A}\sigma(e_1)] \cdot [e] & \text{by def. $T$}\\
    &= [(\LAM{x\of A}\sigma(e_1)) \APP e] & \text{by def. $\cdot$}\\
    &= [\sigma(x\by e)(e_1)] & \text{by ($\beta$)}\\
    &= T\,e_1\,\ext{x}{[e]}{\rho} & \text{by def. $T$}
    \end{align*}
    
  \end{itemize}
  
\end{proof}

\begin{lemma}[Frames are Sound]
  \label{lem:frame-sound}
  Suppose $(M,\cdot)$ is a frame.\\
  If $\vdash e = e'$, then $M \models e = e'$
\end{lemma}


\begin{theorem}[Term Model is Sound \& Complete wrt. Eqn. Theory]\ \\
  Suppose $\Gamma \vdash e : A$ and $\Gamma \vdash e' : A$.\\
  $\vdash e = e'$ iff $T \models e = e'$.
\end{theorem}
\begin{proof}
  We consider each direction of the iff separately.
  \begin{itemize}
  \item $\vdash e = e' \implies T \models e = e'$ \\
    The term model is a frame (Lemma~\ref{lem:term-frame}),
    and frames are sound (Lemma~\ref{lem:frame-sound}).
  \item $T \models e = e' \implies \vdash e = e'$ \\
    Wlog. we assume $\Gamma \subset \mathcal{G}$.
    Let $\rho$ be the identity environment
    \[
    \rho = \{ x \mapsto [x] \mid x \in \mathrm{dom}(\Gamma) \}
    \]
    So we have $\rho \models \Gamma$
    and therefore $T\,e\,\rho = T\,e'\,\rho$. We then calculate
    \[
      [e]
      = [\mathsf{rep}(\rho)(e)]
      = T\,e\,\rho
      = T\,e'\,\rho
      = [\mathsf{rep}(\rho)(e')]
      = [e']
    \]
    from which we conclude that $\vdash e = e'$ .
    
  \end{itemize}
  
\end{proof}


\subsection{Completeness of the Equational Theory}


The application operator for the model $E$ is
\[
   f \cdot d \defeq d' \qquad \text{if } (d,d') \in f
\]

\begin{lemma}
\label{lem:E-frame}
The structure $(E,\cdot)$ is a frame.
\end{lemma}
\begin{proof}
  UNDER CONSTRUCTION
\end{proof}

\begin{definition}
  Suppose $(M_1,\cdot)$ and $(M_2,\cdot)$ are frames.  A \emph{partial
    homomorphism} $h : M_1 \to M_2$ is a type-indexed set of
  surjective partial functions $h_A : M_1\,A \rightharpoonup M_2\,A$
  such that
  \begin{enumerate}
  \item for every $A,B$ and $f \in M_1(A \to B)$,
    either
    \begin{itemize}
    \item for all $a$ in the domain of $h_A$
      \[
      h_B( f \cdot a ) = h_{A\to B}(f) \cdot h_A(a)
      \]
      
    \item or $h^{A\to B}(f)$ is undefined and there is no $g \in
      M_2\,(A\to B)$ that satisfies the above equation.
    \end{itemize}
  %% \item for every $a,b \in M_1\,\NATTY$
  %%   \[
  %%   h_{\NATTY}(a \oplus b) = h_{\NATTY}(a) \oplus h_{\NATTY}(b)
  %%   \]
  \item for every $a \in M_1\,\NATTY$
    \[
    h_{\NATTY}(a) = a
    \]
  \end{enumerate}

\end{definition}


\begin{lemma}[Homomorphisms from $E$ to other frames]
\label{lem:homo-model-frame}
  Suppose $(M,\cdot)$ is a frame. Then there exists a partial
  homomorphism $h : E \to M$.
\end{lemma}
\begin{proof}
  We defined $h$ as follows.
  \begin{align*}
    h_{\NATTY}(n) &= n \\
    h_{A \to B}(f) &= 
    \begin{cases}
      g & \text{if } \forall a \in \mathrm{dom}(h_A), g \cdot h_A(a) = h_B(f \cdot a)\\
      \bot & \text{otherwise }
    \end{cases}
  \end{align*}
  Note that the above definition of $h_{A \to B}$ immediately satisfies
  the requirement that, for any $f \in E(A\to B)$ and $a \in E\,A$
  \[
  h_B(f \cdot a) = h_{A \to B}(f) \cdot h_A(a)
  \]

  But we still need to prove that $h$ is a surjection. We proceed by
  induction on its type index.  It is trivial for $\NATTY$.  Next we
  consider $A \to B$.  Suppose $g \in M\,(A \to B)$. We need to show
  that $h_{A\to B}(g') = g$ for some $g' \in E\,(A\to B)$.  Let $g'$
  be defined as follows.
  \[
  g'(a) = h^{-1}_B(g \cdot h_A(a))
  \]
  The inverse $h^{-1}_B$ is total because, by the induction hypothesis,
  we may assume that $h_B$ is a surjection.
  Now for any $a' \in E\,A$ we have
  \[
  h(g') \cdot h(a') = h(g' \cdot a')
   = h(h^{-1}(g \cdot h(a'))
   = g \cdot h(a')
  \]
  so by extensionality, we conclude that $h_{A\to B}(g') = g$.

\end{proof}

\begin{lemma}
\label{lem:homo-equal}
  Suppose $h : M_1 \to M_2$ is a partial homomorphism, $\Gamma \vdash
  e : A$, $\rho \models \Gamma$, $\rho' \models \Gamma$, and
  $h(\rho(x)) = \rho'(x)$ for every $x \in \mathrm{dom}(\Gamma)$.
  \[
    h(M_1\,e\,\rho) = M_2\,e\,\rho'
  \]
\end{lemma}
\begin{proof}
  The proof is by induction on $e$.
  \begin{itemize}
  \item Case $e=n$:\\
    \[
    h(M_1\,n\,\rho) = h(n) = n = M_2\,n\,\rho'
    \]
  \item Case $e=e_1 \oplus e_2$:
    \begin{align*}
    h(M_1\,(e_1 \oplus e_2)\,\rho) 
    &= h(M_1\,e_1\,\rho \oplus M_1\,e_2\,\rho) & \text{$M_1$ is a frame} \\
    &= h(M_1\,e_1\,\rho) \oplus h(M_1\,e_2\,\rho) & \text{$h$ is a partial homomorphism}\\
    &= M_2\,e_1\,\rho' \oplus M_2\,e_2\,\rho' & \text{by I.H.}\\
    &= M_2\,(e_1 \oplus e_2)\,\rho' & \text{$M_2$ is a frame}
    \end{align*}

  \item Case $e=x$:
    \[
      h(M_1\,x\,\rho) = h(\rho(x)) = \rho'(x) = M_2\,x\,\rho'
    \]
  \item Case $e=\LAM{y \of A_1} e_1$ and $A = A_1 \to A_2$: \\
    We need to prove
    \[
    h(M_1\,(\LAM{y \of A_1} e_1)\,\rho) = M_2\,(\LAM{y \of A_1} e_1)\,\rho'
    \]
    but because $M_2$ is a pre-frame, we have extensionality, so it suffices to
    show that for any $d' \in M_2\,A_1$
    \[
    h(M_1\,(\LAM{y \of A_1} e_1)\,\rho) \cdot d' = (M_2\,(\LAM{y \of A_1} e_1)\,\rho') \cdot d'
    \]
    Let $d = h^{-1}(d')$. 
    \begin{align*}
      h(M_1\,(\LAM{y \of A_1} e_1)\,\rho) \cdot h(d)
      &= h(M_1\,(\LAM{y \of A_1} e_1)\,\rho \cdot d) & \text{$h$ is a partial homomorphism}\\
      &= h(M_1\,e_1\,\ext{y}{d}{\rho}) & \text{$M_1$ is a frame}\\
      &= M_2\,e_1\,\ext{y}{d'}{\rho'} & \text{by I.H.} \\
      &= M_2\,(\LAM{y \of A_1} e_1)\,\rho' \cdot d' & \text{$M_2$ is a frame}
    \end{align*}
    
  \item Case $e=e_1 \APP e_2$:\\
    \begin{align*}
    h(M_1\,(e_1 \APP e_2)\,\rho) 
    &= h(M_1\,e_1\,\rho \cdot M_1\,e_2\,\rho) & \text{$M_1$ is a frame} \\
    &= h(M_1\,e_1\,\rho) \cdot h(M_1\,e_2\,\rho) & \text{$h$ is a partial homomorphism}\\
    &= M_2\,e_1\,\rho' \cdot M_2\,e_2\,\rho' & \text{by I.H.}\\
    &= M_2\,(e_1 \APP e_2)\,\rho' & \text{$M_2$ is a frame}
    \end{align*}

  \end{itemize}
\end{proof}


\begin{lemma}
\label{lem:homo-models}
Suppose there is a partial homomorphism $h : M_1 \to M_2$.
Also, suppose $\Gamma \vdash e : A$ and $\Gamma \vdash e' : A$.
If $M_1 \models e = e'$, then $M_2 \models e = e'$.
\end{lemma}
\begin{proof}
  Let $\rho'$ be an environment s.t. $\rho' \models \Gamma$.  We need
  to show that $M_2\,e\,\rho' = M_2\,e'\,\rho'$.  Define $\rho(x) =
  h^{-1}(\rho'(x))$, so $\rho'(x) = h(\rho(x))$.  ($h$ is a
  surjection, so $h^{-1}$ is total.) Note that $\rho \models \Gamma$.
  \begin{align*}
    M_2\,e\,\rho' 
    &= h(M_1\,e\,\rho) & \text{by Lemma~\ref{lem:homo-equal}}\\
    &= h(M_1\,e'\,\rho) & \text{because } M_1 \models e = e'\\
    &= M_2\,e'\,\rho' & \text{by Lemma~\ref{lem:homo-equal}}
  \end{align*}
\end{proof}

\begin{theorem}[Completeness of Theory wrt. the Model]\ \\
  If $E \models e = e'$, then $\vdash e = e'$.
\end{theorem}
\begin{proof}
  There is a partial homomorphism $h : E \to T$
  (Lemma~\ref{lem:homo-model-frame}).  Then because $E \models e = e'$
  we also have $T \models e = e'$ (Lemma~\ref{lem:homo-models}).
  

\end{proof}


%===============================================================================
\section{Untyped Functions}
\label{sec:lambda}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Siek:2017ab} 
\end{tabular}
}

We now turn to an untyped functional language, the $\lambda$-calculus.
The syntax is nearly the same as that of the simply-typed
$\lambda$-calculus, except that $\lambda$ abstractions no longer come
with a type annotation on their parameter.  Because the
$\lambda$-calculus is untyped, we consider any syntactically
well-formed expression to be in the language (no type system
necessary) and any closed expression is a valid program.  We study
here the call-by-value (CBV) version of the $\lambda$-calculus.  As we
did for the STLC, we include arithmetic on natural numbers.
\[
\begin{array}{lrl}
              & n \in \mathbb{N} \\
 \text{vars.} & x \in \VAR\\
 \text{expr.}& e \in\mathbb{E} ::=& n \mid e + e \mid e \times e \mid x \mid \LAM{x} e \mid (e \, e)
\end{array}
\]

\subsection{Semantics}

In the $\lambda$-calculus, functions are first-class entities, so they
can be passed to other functions and returned from them. This
introduces a difficulty in trying to give a semantics in terms of
regular mathematical sets and functions. It seems that we need a set
$\mathbb{D}$ that solves the following equation.
\[
   \mathbb{D} = \mathbb{N} + (\mathbb{D} \pto \mathbb{D})
\]
But such a set cannot exist because the size of $\mathbb{D} \pto
\mathbb{D}$ is necessarily larger than $\mathbb{D}$!

There are several ways around this problem. One approach is to
consider only continuous functions, which cuts down the size enough to
make it possible to solve the equation. The first model of the
$\lambda$-calculus, $D_\infty$ of \citet{Scott:1970dp}, takes this
approach. We shall study $D_\infty$ in Section~\ref{sec:D-infinity}.

Another approach does not require solving the above equation, but
recognizes that when passing a function to another function, one
doesn't need to pass the entirety of the function (which is infinite),
but one can instead pass a finite subset of the function's graph.  The
trouble of deciding which finite subset to pass can be sidestepped by
trying all possible subsets. This would of course be prohibitive if
our current goal was to implement the $\lambda$-calculus, but this
``inefficiency'' does not pose a problem for a semantics, a
\emph{specification}, of the $\lambda$-calculus. The various semantics
that take this approach are called \emph{graph models}. This first
such model was $T^{*}_C$ of \citet{Plotkin:1972aa}. Then came
$\mathcal{P}(\omega)$ of \citet{Scott:1976lq} and the simpler $D_A$ of
\citet{Engeler:1981aa}. We shall study these three models in
Section~\ref{sec:graph-models}. But first, we study perhaps the most
straightforward of the graph models, the recent one by yours
truly~\citep{Siek:2017ab}. (todo: What about filter models? And isn't
Siek's in some sense closer to the filter models?)

\begin{figure*}
\begin{minipage}{0.5\textwidth}
Domain
\[
\begin{array}{lrl}
               & n \in \mathbb{N} \\
  \text{elts.} & d \in \mathbb{D} ::= &n \mid \{ (d_1,d'_1), \ldots, (d_n,d'_n) \} \\
  \text{env.} & \rho \in  \VAR \pto \mathbb{D} 
\end{array}
\]
Ordering
\[
   n \sqsubseteq n \qquad 
   \inference{t \subseteq t'}{t \sqsubseteq t'}
\]
Application
\begin{align*}
  %% \uplambda^S & : (\mathbb{D} \to \mathcal{P}(\mathbb{D})) \to \mathcal{P}(\mathbb{D}) \\
  %% \uplambda^S f &\defeq
  %%     \{ t \mid \forall (d,d') \in t, d' \in f\, d \} \\[1ex]
  \_ \cdot \_ &: \mathcal{P}(\mathbb{D}) \times \mathcal{P}(\mathbb{D})\to \mathcal{P}(\mathbb{D}) \\
  D^{*}_1 \cdot D^{*}_2 & \defeq
    \left\{ d_3 \middle|
    \begin{array}{l}
      \exists t d_2 d d', t \in D^{*}_1,  d_2 \in D^{*}_2, \\
      (d,d') \in t, d \sqsubseteq d_2, d_3 \sqsubseteq d'
    \end{array}
    \right\}
\end{align*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
Semantics
\begin{align*}
  P\,e &\defeq \{ d \mid d \in E\,e\,\emptyset\} \\[1ex]
  E & : \mathbb{E} \to (\VAR \pto \mathbb{D}) \to \mathcal{P}(\mathbb{D})\\
  E\, n \, \rho & = \{ n \} \\
  E\, (e_1 + e_2) \, \rho & = \{ n_1 + n_2 \mid n_1 \in E\,e_1\,\rho, 
                                    n_2 \in E\,e_2\,\rho \} \\
  E\, (e_1 \times e_2) \, \rho & = \{ n_1 n_2 \mid n_1 \in E\,e_1\,\rho, 
                                    n_2 \in E\,e_2\,\rho \} \\
  E\, x \,\rho &= \{ d \mid d \sqsubseteq \rho\,x \} \\
  E\, (\LAM{x}e)\,\rho &= 
      \{ t \mid \forall (d,d') \in t, d' \in E\,e\,\ext{x}{ d}{\rho}) \} \\
  E\, (e_1\,e_2)\,\rho &= (E\,e_1\,\rho) \cdot (E\,e_2\,\rho)
\end{align*}
\end{minipage}
\caption{Semantics of the $\lambda$-calculus}
\label{fig:lambda}
\end{figure*}

Figure~\ref{fig:lambda} defines the syntax and semantics of a CBV
$\lambda$-calculus. To enable trying all possible finite subsets of a
function's graph, the semantics is non-deterministic. That is, the
semantic function $E$ maps programs to sets of elements, instead of a
single element. In the case when an expression produces a natural
number, the set is just a singleton of that number. However, when the
expression produces a function, the set contains all finite
approximations of the function.

The set $\mathbb{D}$ consists of just the natural numbers and finite
graphs (association tables) of functions.  $\mathbb{D}$ is defined
inductively, or equivalently, it is the least solution of
\begin{equation}
   \mathbb{D} = \mathbb{N} + \mathcal{P}_f(\mathbb{D} \times \mathbb{D})
  \label{eq:D}
\end{equation}
Let $t$ range over the finite graphs, that is, $t \in
\mathcal{P}_f(\mathbb{D} \times \mathbb{D})$, $D$ range over finite
sets of elements from $\mathbb{D}$, and $D^{*}$ range over possibly
infinite sets of elements from $\mathbb{D}$.

The semantics in Figure~\ref{fig:lambda} says the meaning of an
abstraction $(\LAM{x}e)$ is the set of all tables $t$ such that each
input-output entry $(d,d')$ makes sense for the $\lambda$. That is,
$d' \in E\,e\,\ext{x}{ d}{\rho}$. 
%% While this is straightforward to
%% express directly, we make use of an auxilliary function $\uplambda^S$,
%% also defined in Figure~\ref{fig:lambda}, which is useful in connecting
%% this semantics to the notions of $\lambda$-models and reflexive cpos
%% from the literature. 
%% [I'm not so sure about this anymore -Jeremy]
%% So far we have discussed two of the three
%% lambda's on the line that gives meaning to abstractions: $\lambda$ is
%% the syntactic entity in $\mathbb{E}$ and $\uplambda^S$ is the helper
%% function defined in ~\ref{fig:lambda}.  The third lambda, the
%% $\uplambda$ in $(\uplambda d. E\,e\,\ext{x}{ d}{\rho})$, is the
%% notation we use for defining a good-old mathematical function.  (The
%% author thinks in HOL, so he really has the lambda of
%% HOL~\cite{Nipkow:2002jl} in mind.)

The semantics of application is defined in terms of the auxilliary
application operator $D^{*}_1 \cdot D^{*}_2$, which applies all the
tables in $D^{*}_1$ to all the elements (arguments) in $D^{*}_2$,
collecting all of the results into a resulting set.  A naive
definition of the application operator is given below. The idea is to
collect up all the results $d'$ that come from matching an argument $d
\in D^{*}_2$ with an entry $(d,d')$ from a table $t \in D^{*}_1$.
\begin{align*}
  D^{*}_1 \cdot D^{*}_2 &= 
  \{ d' \mid
  \exists t d.\, t \in D^{*}_1, d \in D^{*}_2, 
    (d,d') \in t
  \}
  & \text{(naive)}
\end{align*}
The problem with this version is that it prohibits self application,
which is an important part of the $\lambda$-calculus.  For example, it
is necessary to define the $Y$ combinator and thereby express general
recursion. The problem is that in self application, one would need a
table to be inside itself, that is, $(t,d') \in t$.  But that can't
happen because the domain $\mathbb{D}$ is inductively defined. The
solution is to allow the argument to be a larger table than the input
$d$ of the table entry. We define the approximation ordering
$\sqsubseteq$ in Figure~\ref{fig:lambda}.  So then, for argument $d_2
\in D^{*}_2$, we require $d \sqsubseteq d_2$ instead of $d = d_2$.

Say something about $d'$ and $d_3$ to motivate the following.

\[
X \vdash \rho \sqsubseteq \rho' \defeq
  \forall x, x \in X \implies \rho(x) \sqsubseteq \rho'(x)
\]

\begin{proposition}[Subsumption]\ \\
  \label{prop:sub}
  If $e \in E\,e\,\rho$, $d' \sqsubseteq d$,
  and $\mathrm{FV}(e) \vdash \rho \sqsubseteq \rho'$,
  then $d' \in E\,e\,\rho'$.
\end{proposition}

\subsection{Interpreter}

We turn to an implementation of the $\lambda$-calculus, the
interpreter in Figure~\ref{fig:interp-lambda}. The interpreter is
based on the notion of a closure, which pairs a $\lambda$ abstraction
with its environment to ensure that the free variables get their
definitions from the lexical scope.

\begin{figure}
\[
\begin{array}{lrl}
 \text{values} & v \in \mathbb{V} ::= & n \mid \langle \LAM{x}e, \varrho \rangle \\
 \text{env.} & \varrho \in \VAR \pto \mathbb{V} 
\end{array}
\]
\begin{align*}
  I\,n\,\varrho &= n \\
  I\,(e_1 + e_2)\,\varrho &= n_1 + n_2 \\
      & \text{if } I\,e_1\,\varrho = n_1, I\,e_2\,\varrho = n_2 \\
  I\,(e_1 \times e_2)\,\varrho &= n_1 n_2 \\
      & \text{if } I\,e_1\,\varrho = n_1, I\,e_2\,\varrho = n_2 \\
  I\,x\,\varrho &= \varrho(x) \\
  I\,(\LAM{x}e)\,\varrho &= \langle \LAM{x}e, \varrho \rangle \\
  I\,(e_1\,e_2)\,\varrho &=  I\,e\,[x\mapsto v]\varrho'\\
  & \text{if } I\,e_1\,\varrho = \langle \LAM{x}e,\varrho' \rangle,
      I\,e_2\,\varrho = v
\end{align*}

\caption{Interpreter for the $\lambda$-calculus}
\label{fig:interp-lambda}
\end{figure}

We prove the correctness of the interpreter in two steps.  First we
show that if the semantics says the result of a program should be an
number $n$, then the interpreter also produces $n$. The second part is
to show that if the interpreter produces an answer $n$, then the
semantics agrees that $n$ should be the answer.

For the first part, we need to relate denotations (elements $d$) to
the values $v$ used by the interpreter.
\begin{align*}
  \mathcal{G}(n) &= \{ n \} \\
  \mathcal{G}(t) &= \left\{ \langle \LAM{x} e, \varrho \rangle \middle|
     \begin{array}{l}
       \forall (d,d') \in t, v \in \mathcal{G}(d),\\
       \exists v', I\,e\,\ext{x}{v}{\varrho} = v'
               \text{ and } v'\in\mathcal{G}(d')
     \end{array} \right\}
\end{align*}
Similarly, we related semantic environments to the
interpreter's environments.
\[
  \inference{}{\mathcal{G}(\emptyset,\emptyset)}
  \qquad
  \inference{v \in \mathcal{G}(d) & \mathcal{G}(\rho,\varrho)}
            {\mathcal{G}([x\mapsto d] \rho, [x\mapsto v] \varrho}
\]

\noindent Leading up to the first theorem, we establish the following lemmas.

\begin{lemma}[$\mathcal{G}$ is downward closed]
\label{lem:sub-good}
If $v \in \mathcal{G}(d)$ and $d' \sqsubseteq d$, then $v \in \mathcal{G}(d')$.
\end{lemma}

\begin{lemma}
\label{lem:lookup-good}
  If $\mathcal{G}(\rho,\varrho)$,
  then $\varrho(x) \in \mathcal{G}(\rho(x))$
\end{lemma}

\begin{lemma}
  If $d \in E\,e\,\rho$ and $\mathcal{G}(\rho,\varrho)$, then
  $I\,e\,\varrho = v$, $v \in \mathcal{G}(d)$ for some $v$.
\end{lemma}

\begin{theorem}[Adequacy]
If $E\,e\,\emptyset = E\,n\,\emptyset$, then $I\,e\,\emptyset = n$.
\end{theorem}


{\color{red}[The rest of this subsection is under construction! -Jeremy]}

%% \begin{theorem}
%%   $\mathcal{P}(\mathbb{D})$ is a reflexive cpo by
%%   choosing $F=\uplambda x.\uplambda y. x \cdot y$ and
%%   $G=\uplambda^S$.

%% \end{theorem}
%% \begin{proof}
%%   TODO: prove that $F$ and $G$ are continuous and that
%%   $(F \circ G)\, f = f$.

%% \begin{align*}
%%   (F \circ G)\, f &= \uplambda y. (\uplambda^S f) \cdot y \\
%%      &= \uplambda y. \{ d' \mid \exists t d d_2,
%%             t \in (\uplambda^S f), d_2 \in y, (d,d') \in t, d \sqsubseteq d_2 \}\\
%%      &= \uplambda y. \{ d' \mid \exists t d d_2,
%%             (\forall (d_0,d'_0) \in t, d'_0 \in f d_0),
%%             d_2 \in y, (d,d') \in t, d \sqsubseteq d_2 \}\\
%%      &= \uplambda y. \{ d' \mid \exists d d_2,
%%             d_2 \in y, d' \in f\, d, d \sqsubseteq d_2 \}\\
%%      & \vdots \\
%%      &= \uplambda y. \{ d' \mid ... \}\\
%%      &= f
%% \end{align*}

%% \end{proof}

\begin{theorem}[Completeness wrt. Interpreter]
If $I\,e\,\emptyset = n$, then $E\,e\,\emptyset = E\,n\,\emptyset$.
\end{theorem}

Toward proving completeness, we introduce a memoizing interpreter,
that is, an interpreter that records the input and output of every
function call in a table.


\[
\begin{array}{lrl}
 \text{fun. id's} & f \in \mathbb{N} \\
 \text{values} & v \in \mathbb{V} ::= & n \mid f \\
 \text{env.} & \varrho \in \VAR \pto \mathbb{V} \\
             & t ::= & \{ (v_1,v'_1), \ldots, (v_n,v'_n) \}\\
 \text{memo} & m \in \mathbb{M} ::= & \langle \LAM{x}e, \varrho, t \rangle\\
 \text{memo table} & T \in & \mathbb{N} \pto \mathbb{M}
\end{array}
\]

The following produces cyclic tables. How do we convert
these into non-cyclic tables? Or can we generate non-cyclic
tables to begin with? Or put another way, how to we record
the callee-view of a memo. table? -Jeremy
\begin{align*}
  I'\,n\,\varrho\,T &= (n,T) \\
  I'\,(e_1 + e_2)\,\varrho\,T &= (n_1 + n_2,T_2) \\
      & \text{if } I'\,e_1\,\varrho\,T = (n_1,T_1), I'\,e_2\,\varrho\,T_1 = (n_2,T_2) \\
  I'\,(e_1 \times e_2)\,\varrho\,T &= (n_1 n_2,T_2) \\
      & \text{if } I'\,e_1\,\varrho\,T = (n_1,T_1), I'\,e_2\,\varrho\,T_1 = (n_2,T_2) \\
  I'\,x\,\varrho\,T &= (\varrho(x),T) \\
  I'\,(\LAM{x}e)\,\varrho\,T &= (|T|, 
     \ext{|T|}{\langle \LAM{x}e, \varrho, \emptyset \rangle}{T} \\
  I'\,(e_1\,e_2)\,\varrho\,T &= (v',\ext{f}{\langle \LAM{x}e,\varrho',\{(v,v')\}\cup t \rangle}{T_3}) \\
  & \text{if } I'\,e_1\,\varrho\,T = (f,T_1),
      I'\,e_2\,\varrho\,T_1 = (v,T_2), \\
    &  T(f) = \langle \LAM{x}e,\varrho',t \rangle,
      I'\,e\,[x\mapsto v]\varrho'\,T_2 = (v',T_3)
\end{align*}


\subsection{Reduction Semantics}


[TODO: define reduction semantics]


\begin{lemma}[Invariance under Substitution]
\[
  E\,[x\by v]e\,\rho = \bigcup_{d' \in E\,v\,\emptyset} E\,e\,\ext{x}{d'}{\rho}
\]
\end{lemma}

\begin{proposition}[Invariance under reduction]\ \\
\noindent If $e \longrightarrow e'$, then $\forall \rho, E\,e\,\rho =
E\,e'\,\rho$.
\end{proposition}

\begin{theorem}[Completeness wrt. reduction]\ \\
\noindent If $e \longrightarrow^{*} n$, then $E\,e\,\emptyset =
E\,n\,\emptyset$.
\end{theorem}

\begin{definition}[Contexts]
\[
  C ::= \Box \mid C + e \mid e + C \mid C \times e \mid e \times C \mid
       \LAM{x} C \mid C \APP e \mid e \APP C
\]
\end{definition}

\begin{proposition}[Congruence]\ \\
  If $\forall \rho, E\,e\,\rho = E\,e'\,\rho$, 
  then $\forall \rho, E\,C[e]\,\rho = E\,C[e']\,\rho$
  for any $C$.
\end{proposition}

We write $e\Downarrow$ to mean that $e$ terminates, i.e., it reaches a
value or gets stuck after some number of reduction steps.

\begin{theorem}[Sound wrt. contextual equivalence]\ \\
  If $\forall \rho, E\,e\,\rho = E\,e'\,\rho$, 
  then $C[e]\Downarrow$ iff $C[e']\Downarrow$ for any closing
  context $C$.
\end{theorem}


\subsection{Equational Theory and Models}

Figure~\ref{fig:lambda-eq} defines the equational theory of two
variants of the $\lambda$-calculus, the full $\lambda$-calculus with
the rule $\beta$, and the call-by-value calculus with the rule
$\beta_v.$

\begin{figure}
\begin{gather*}
  \text{(refl)}  \vdash e = e
  \quad
  \text{(sym)}   \inference{\vdash e_1 = e_2}{\vdash e_2 = e_1}  
  \quad
  \text{(trans)}  \inference{\vdash e_1 = e_2 & \vdash e_2 = e_3}{\vdash e_1 = e_3} \\[2ex]
  \text{(cong)}  \inference{\vdash e_1 = e_3 & \vdash e_2 = e_4}
                            {\vdash e_1 \APP e_2 = e_3 \APP e_4}
   \quad
  \text{($\xi$)}  \inference{\vdash e = e'}{\vdash \LAM{x}e = \LAM{x}e'} \\[2ex]
  \text{($\beta$)}  \vdash (\LAM{x}e_1)\APP e_2 = [x\by e_2]e_1 \\[2ex]
  \text{($\beta_v$)}  \vdash (\LAM{x}e)\APP v = [x\by v]e
\end{gather*}
\caption{Equational Theory of the $\lambda$-calculus}
\label{fig:lambda-eq}
\end{figure}


\paragraph{Direct Proof}

[TODO: direct proofs that $E$ satisfies the equational theory based on
  prior theorems about invariance under reduction and congruence.]

\paragraph{Proof via $\lambda$-model}

The following notion of $\lambda$-model is used in the literature that
captures a set of conditions that are sufficient for a model to
satisfy the equational theory of the $\lambda$-calculus.

\begin{definition}[$\lambda$-models]
  An \emph{$R$-model of the $\lambda$-calculus} is a triple $\langle
  D, \cdot, \SEM{\ }\rangle$ such that $D$ is a set, $\_\cdot\_:D \times D
  \to D$, and $\SEM{\_} : \mathbb{E}\to(\VAR\pto D)\to D$, and the
  following conditions hold.
  \begin{enumerate}
  \item $\SEM{ x}\rho = \rho(x)$, \label{cond:var}
  \item $\SEM{e_1\APP e_2}\rho = \SEM{e_1}\rho \cdot \SEM{e_2}\rho$ \label{cond:app},
  \item if $(\LAM{x}e) \APP e'$ is a $R$-redex, $\SEM{(\LAM{x}e)}\rho \cdot \SEM{e'}\rho = \SEM{e}\ext{x}{\SEM{e'}\rho}{\rho}$\label{cond:R}, \\
        alternative: $\SEM{(\LAM{x}e)}\rho \cdot d = \SEM{e}\ext{x}{ d}{\rho}$ \label{cond:lam-app}
  \item $\forall x \in \mathrm{FV}(e)$, if  $\rho(x) = \rho'(x)$,
       then $\SEM{e}\rho = \SEM{e}\rho'$. \label{cond:env}
  \item If $\forall d \in D, \SEM{e}\ext{x}{ d}{\rho} = \SEM{e'}\ext{x}{ d}{\rho}$,
       then $\SEM{\LAM{x} e}\rho = \SEM{\LAM{x} e'}\rho$. \label{cond:lam}
  \end{enumerate}
\end{definition}

The semantic function $E$ given in Figure~\ref{fig:lambda} does not
directly fit the above definition of $\lambda$-model because its
environment maps variables to $\mathbb{D}$ instead of
$\mathcal{P}(\mathbb{D})$. However, this difference can be bridged
with the following alternative semantic function, based on an
analogous construction for filter models~\citep{Alessi:2006aa}, which
we discuss in Section~\ref{sec:filter-models}.

\begin{align*}
  E'\,e\,\rho' &=
  \{ d \mid \exists \rho, d \in E\,e\,\rho
  \text{ and } \rho \models \rho' \}
\end{align*}
where
\[
\rho\models\rho' \defeq \forall x \in\mathrm{dom}(\rho),\rho(x)\in \rho'(x)
\]
Actually, for the proof to go through, we can't let environments map
variables to arbitrary sets, many of which couldn't possibly be the
denotation of an expression. Such sets are missing two important
properties of actual denotations; they are 1) downward closed with
respect to $\sqsubseteq$, and 2) closed under $\sqcup$. A set with
these closure conditions is called an \emph{ideal}. We write
$\mathcal{I}(\mathbb{D})$ for the ideals over $\mathbb{D}$.  So $E'$
has the following type.
\[
  E' : \mathbb{E} \to (\VAR\pto \mathcal{I}(\mathbb{D}))
  \to \mathcal{I}(\mathbb{D}) 
\]

\begin{lemma}
$\langle \mathcal{I}(\mathbb{D}), \cdot, E')$ is a $\beta_v$-model of
  the $\lambda$-calculus.
\end{lemma}
\begin{proof}\ 
\begin{enumerate}
\item We need to show that $E'\,x\,\rho' = \rho'(x)$.
  \begin{align*}
    E'\,x\,\rho' &=
    \{ d \mid \exists \rho, d \in E\,x\,\rho, \rho \models \rho'\}\\
    &= \{ d \mid \exists \rho, \rho(x)=d, d \in \rho'(x) \} \\
    &= \{ d \mid d \in \rho'(x) \} \\
    &= \rho'(x)
  \end{align*}
\item We need to show that
      $E'\,(e_1\APP e_2)\,\rho' = (E'\,e_1\,\rho') \cdot (E'\,e_2\,\rho')$.
  \begin{align*}
    E'\,(e_1\APP e_2)\,\rho' &=
    \{ d' \mid \exists \rho, d' \in E\,(e_1\APP e_2)\,\rho,
    \rho \models \rho'\} \\
    &= \{ d' \mid \exists \rho, d' \in (E\,e_1\,\rho) \cdot (E\,e_2\,\rho),
    \rho \models \rho' \} \\
    &= \{ d' \mid \exists  t d d_2 \rho,
    t \in E\,e_1\,\rho,  d_2 \in E\,e_2\,\rho, 
    (d,d') \in t, d \sqsubseteq d_2,
    \rho \models \rho' \} \\
    &= \{ d' \mid
    \exists t d d_2,
    (\exists \rho, t \in E\,e_1\,\rho,\rho\models\rho'),
    (\exists \rho, d_2 \in E\,e_2\,\rho,\rho \models \rho'),
    (d,d') \in t, d \sqsubseteq d_2
    \} \\
    & \qquad \text{by Proposition~\ref{prop:sub} and Lemma~\ref{lem:env-less-model}}\\
    &= \{ d' \mid
    \exists t d d_2, t \in E'\,e_1\,\rho',  d_2 \in E'\,e_2\,\rho',
    (d,d') \in t, d \sqsubseteq d_2
    \} \\
    &= (E'\,e_1\,\rho') \cdot (E'\,e_2\,\rho')
  \end{align*}
\item
  \begin{align*}
    E'\,(\LAM{x}e)\,\rho' \cdot D^{*} &=
    \{ t \mid \exists \rho, \rho\models\rho',
          \forall (d,d)\in t, d' \in E\,e\,\ext{x}{d}{\rho} \} \cdot D^{*} \\
   &= \{ d_3 \mid \exists d_2 d d' \rho,
                  \rho\models\rho',
                   d' \in E\,e\,\ext{x}{d}{\rho},
                  d_2 \in D^{*}, d \sqsubseteq d_2, d_3 \sqsubseteq d' \} \\
   &= \{ d \mid \exists \rho, d \in E\,e\,\rho, \rho \models \ext{x}{D^{*}}{\rho'}\} \\
   & \qquad\text{because } \ext{x}{d}{\rho}\models\ext{x}{D^{*}}{\rho'},
      d_3 \in E\,e\,\ext{x}{d}{\rho} \\
   &= E'\,e\,\ext{x}{D^{*}}{\rho'}
  \end{align*}
\item TODO
\item TODO
\end{enumerate}
\end{proof}

\begin{lemma}
  \label{lem:env-less-model}
  If $\rho_1 \models \rho'$, $\rho_2 \models \rho'$,
  then $\rho_1 \sqcup \rho_2 \models \rho'$.
\end{lemma}
\begin{proof}
  TODO This is where we probably need the requirements of
  being closed under union (ideal), dual to filters.
\end{proof}

\begin{lemma}[Replace Variable]
  \label{lem:change-var}
  Suppose $\langle D,\cdot, \SEM{\ } \rangle$ is an $R$-model of the
  $\lambda$-calculus and $y \notin \mathrm{FV}(e)$.
  \[
    \SEM{e} \ext{x}{ d}{\rho}
    = 
    \SEM{[x\by y]e} \ext{y}{ d}{\rho}
  \]
\end{lemma}
\begin{proof}
  \begin{align*}
    \SEM{e} \ext{x}{ d}{\rho} &= \SEM{\LAM{x}e}\rho \cdot d & \text{by condition \ref{cond:lam-app}}\\
       &= \SEM{\LAM{y}[x\by y]e}\rho \cdot d & \text{$\alpha$ conversion}\\
       &= \SEM{[x\by y]e} \ext{y}{ d}{\rho} & \text{by condition \ref{cond:lam-app}}
  \end{align*}
\end{proof}

\begin{lemma}[Substitution] (5.3.3 of \citet{barendregt84:_lambda_calculus} )
\label{lem:subst-model}
  Suppose $\langle D,\cdot, \SEM{\ } \rangle$ is an $R$-model of the
  $\lambda$-calculus. 
  \begin{enumerate}
  \item If $z \notin \mathrm{FV}(e)$, then
      $\forall \rho, \SEM{ [x\by z]e } \rho = \SEM{e}\ext{x}{\SEM{z}\rho}{\rho}$.
  \item If $\forall \rho, \SEM{ [x\by e']e} \rho = \SEM{e}\ext{x}{\SEM{e'}\rho}{\rho}$, \\
    then $\forall \rho, \SEM{ [x\by e']\LAM{y}e} \rho = \SEM{\LAM{y}e}\ext{x}{\SEM{e'}\rho}{\rho}$.
  \item $\forall \rho, \SEM{ [x\by e']e} \rho = \SEM{e}\ext{x}{\SEM{e'}\rho}{\rho}$.
  \end{enumerate}
\end{lemma}
\begin{proof}\
  We prove part 1, then part 2 using part 1, and finally part 3 using
  part 2 of this lemma.
\begin{enumerate}
\item Assume $z \notin \mathrm{FV}(e)$. Let $e \equiv e(x)$.
  \begin{align*}
    \SEM{ e(z) } \rho &= \SEM{e(z)}[z\by \rho(z)]\rho & \text{by condition \ref{cond:env}}\\
       &= \SEM{e(x)}[x\by \rho(z)]\rho & \text{by Lemma~\ref{lem:change-var}}\\
       &= \SEM{e(x)}[x\by \SEM{z}\rho]\rho  & \text{by condition \ref{cond:var}}
  \end{align*}
\item 
  To prove this implication, we assume its premise:
  \begin{equation}
    \forall \rho, \SEM{ [x\by e']e} \rho = \SEM{e}\ext{x}{\SEM{e'}\rho}{\rho} \label{eq:phi-subst}
  \end{equation}
  Let $\rho$ be an arbitrary environment.
  We consider two cases, whether $x \in \mathrm{FV}(e')$ or not.
  \begin{itemize}
  \item Assume $x \in \mathrm{FV}(e')$. Let $z$ be a fresh variable.
    \begin{align*}
      \SEM{[x\by e']\LAM{y}e}\rho &= \SEM{ [z\by e'][x\by z]\LAM{y}e }\rho\\
             &= \SEM{[x\by z]\LAM{y}e} \ext{z}{\SEM{e'}\rho}{\rho} & \text{??} \\
             &= \SEM{\LAM{y}e}\ext{x}{\SEM{e'}\rho}{\ext{z}{\SEM{e'}\rho}{\rho}}& \text{by part 1}\\
             &= \SEM{\LAM{y}e}\ext{x}{\SEM{e'}\rho}{\rho}
    \end{align*}

  \item Assume $x \notin \mathrm{FV}(e')$.
    Let $d \in D$ be an arbitrary element.
    \begin{align}
           \SEM{[x\by e']e}\ext{y}{d}{\ext{x}{\SEM{e'}\rho}{\rho}} \notag
        &= \SEM{[x\by e']e}\ext{y}{d}{\rho} & \text{cond. \ref{cond:env}}\notag\\
        &= \SEM{e}\ext{x}{\SEM{e'}\rho}{\ext{y}{d}{\rho}} & \text{by eq. \eqref{eq:phi-subst}} \notag\\
        &= \SEM{e}\ext{y}{d}{\ext{x}{\SEM{e'}\rho}{\rho}} & x\neq y \label{eq:body}
    \end{align}
    \begin{align*}
      \SEM{ [x\by e']\LAM{y}e} \rho
      &= \SEM{ [x\by e']\LAM{y}e} \ext{x}{\SEM{e'}\rho}{\rho}
         & \text{condition \ref{cond:env}} \\
      &= \SEM{ \LAM{y}[x\by e'] e} \ext{x}{\SEM{e'}\rho}{\rho} \\
      &= \SEM{\LAM{y}e}\ext{x}{\SEM{e'}\rho}{\rho}
          & \text{cond. \ref{cond:lam} and eq. \eqref{eq:body}}
    \end{align*}
    
    
  \end{itemize}
  
\item The proof is by induction on $e$.
  \begin{itemize}
  %% \item[Case $e=n$:]
  %%   $\SEM{[x\by e']n}\rho = \SEM{n}\rho = \SEM{n}\ext{x}{\SEM{e'}\rho}{\rho}$
  %%   by condition \ref{cond:env}
  %% \item[Case $e=e_1 + e_2$:]
  %%   \begin{align*}
  %%     \SEM{[x\by e'](e_1 + e_2)}\rho
  %%     &= \SEM{[x\by e']e_1 + [x\by e']e_2}\rho \\
  %%     &= ...
  %%   \end{align*}
  %% \item[Case $e=e_1 \times e_2$:] (analogous to the above case)
  \item[Case $e=y$:]
    If $y \neq x$, then
    $\SEM{[x\by e']y}\rho = \rho(y) = \SEM{y}\ext{x}{\SEM{e'}\rho}{\rho}$.\\
    If $y = x$, then
    $\SEM{[x\by e']y}\rho = \SEM{e'}\rho = \SEM{y}\ext{x}{\SEM{e'}\rho}{\rho}$.
  \item[Case $e=\LAM{y}e_1$:]
    By the IH we have
    \[
    \forall \rho, \SEM{ [x\by e']e_1} \rho = \SEM{e_1}\ext{x}{\SEM{e'}\rho}{\rho}
    \]
    Then by part 2 of this lemma we conclude that
    \[
    \forall \rho, \SEM{[x\by e']\LAM{y}e_1}\rho = \SEM{\LAM{y}e_1}\ext{x}{\SEM{e'}\rho}{\rho}
    \]
    
  \item[Case $e=e_1\APP e_2$:]
    Let $\rho$ be an arbitrary environment.
    \begin{align*}
      \SEM{[x\by e'](e_1 \APP e_2)} \rho
      &= \SEM{[x\by e']e_1 \APP [x\by e']e_2} \rho \\
      &= \SEM{[x\by e']e_1}\rho \cdot \SEM{[x\by e']e_2} \rho
         & \text{by cond. \ref{cond:app}}\\
      &= \SEM{e_1}\ext{x}{\SEM{e'}\rho}{\rho}
          \cdot \SEM{e_2}\ext{x}{\SEM{e'}\rho}{\rho}
         & \text{by IH}\\
      &= \SEM{e_1 \APP e_2}\ext{x}{\SEM{e'}\rho}{\rho}
         & \text{by cond. \ref{cond:app}}
    \end{align*}
    
    
  \end{itemize}
  
\end{enumerate}
\end{proof}

\begin{lemma}[Soundess of R-models wrt. $\lambda$ theory]
  Suppose $\langle D,\cdot,\SEM{\ }\rangle$ is an $R$-model of the
  $\lambda$-calculus.
  \[
  \text{if } R \vdash e = e', \text{ then } \forall \rho, \SEM{e}\rho = \SEM{e'}\rho
  \]
\end{lemma}
\begin{proof}
  The proof is by induction on $R \vdash e = e'$.
  \begin{itemize}
  \item[Case (refl):] We have $\forall \rho, \SEM{e}\rho = \SEM{e}\rho$
    because $\SEM{\ }$ is a function.
    
  \item[Case (sym):]
    By the IH, we have $\forall \rho, \SEM{e_1}\rho = \SEM{e_2}\rho$.
    So it trivially follows that $\forall \rho, \SEM{e_2}\rho = \SEM{e_1}\rho$.
    
  \item[Case (trans):]
    By the IH, we have $\forall \rho, \SEM{e_1}\rho = \SEM{e_2}\rho$
    and $\forall \rho, \SEM{e_2}\rho = \SEM{e_3}\rho$.
    Thus, we immediately have $\forall \rho, \SEM{e_1}\rho = \SEM{e_3}\rho$.
    
  \item[Case (cong):]
    Let $\rho$ be an arbitrary environment.
    We need to show that $\SEM{(e_1\APP e_2)}\rho = \SEM{(e_3\APP e_4)}\rho$.
    \begin{align*}
      \SEM{e_1\APP e_2}\rho &= \SEM{e_1}\rho \cdot \SEM{e_2}\rho & \text{condition \ref{cond:app}}\\
        &= \SEM{e_3}\rho \cdot \SEM{e_4}\rho & \text{by IH}\\
        &= \SEM{e_3\APP e_4}\rho & \text{condition \ref{cond:app}}
    \end{align*}
    
  \item[Case $\xi$:]
    We need to show $\forall \rho, \SEM{\LAM{x}e}\rho = \SEM{\LAM{x}e'}\rho$.
    Let $\rho$ be an arbitrary environment.
    By condition \ref{cond:lam} it suffices to show that
    $\SEM{e}\ext{x}{d}{\rho} = \SEM{e'}\ext{x}{d}{\rho}$,
    and that follows from the IH.

  \item[Case $R$:]
    Let $\rho$ be an arbitrary environment.
    We need to show that
    $\SEM{(\LAM{x}e)\APP e'}\rho = \SEM{[x\by e']e}\rho$,
    where $(\LAM{x}e)\APP e'$ is an $R$-redex.
    \begin{align*}
      \SEM{(\LAM{x}e)\APP e'}\rho &=
      \SEM{\LAM{x}e}\rho \cdot \SEM{e'}\rho & \text{condition \ref{cond:app}} \\
      &= \SEM{e} \ext{x}{\SEM{e'}\rho}{\rho} & \text{condition \ref{cond:R}} \\
      &= \SEM{[x\by e']e}\rho & \text{by Lemma~\ref{lem:subst-model}}
    \end{align*}
    
  \end{itemize}
\end{proof}


\begin{theorem}
$\langle \mathcal{I}(\mathbb{D}), \cdot, E')$ is a model of the CBV
  $\lambda$-calculus. That is, 
  \[
  \text{if } \beta_v \vdash e = e', 
  \text{ then } \forall \rho, E'\,e\,\rho = E'\,e'\,\rho
  \]
\end{theorem}


Resources:
\begin{itemize}
\item \citet{barendregt84:_lambda_calculus} (Section 2.1 for
  equational theory and Section 5.3 for (syntactical) $\lambda$-models).
\item \citet{Alessi:2006aa}
\item \citet{Hindley:1980aa}
\end{itemize}

%===============================================================================

\subsection{Intersection Types and Filter models}
\label{sec:filter-models}

\newcommand{\UP}[1]{\mathop{\uparrow}\! #1}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Alessi:2006aa}
\end{tabular}
}

The addition of intersection types to a type system dramatically
increases its precision. In fact, intersection types are fully
precise; an intersection type system specifies the dynamic semantics
of a program.
%
There are a large number of different but related intersection type
systems. \citet{Alessi:2006aa} give a survey of them and prove general
results that relate them to equational theories of
$\lambda$-calculi. Here we focus on just one of them, a variant of the
$\mathcal{EHR}$ system for the call-by-value $\lambda$-calculus
of~\citet{Egidi:1992aa}. Figure~\ref{fig:intersection-types} defines
types and the subtyping and type equivalence rules.  With respect to
\citet{Egidi:1992aa}, here we have added a singleton type $n$ for
natural numbers. 

The intuition behind the typing rules for intersection types is that
if we think of types as sets of values, subtyping acts like set
inclusion ($\subseteq$), and intersection is like set
intersection. So, for example, the rule (incl-L) corresponds to an
obviously true statement about sets, that $A \cap B \subseteq A$.

The type $\nu$ characterizes all functions, so the ($\nu$) subtyping
rule says that any function type is a subtype of $\nu$. The rule
($\to{-}\cap$) captures the intuition that if you call a function of
intersection type $(A \to B) \cap (A \to C)$ with an argument of type
$A$, we can use $A \to B$ to deduce that the result is in $B$ and use
$A \to C$ to deduce that the result is also in $C$. Thus, the result
must be in $B \cap C$. The ($\eta$) subtyping rule is the standard one
for function types.

\begin{figure}
\[
\begin{array}{lrl}
  \text{types} & A,B,C \in \mathbb{T} ::= & n \mid \nu \mid A \to B \mid A \cap B
\end{array}
\]
\[
\begin{array}{rlrl}
  \text{(refl)} &  A <: A & \text{(idem)} & A <: A \cap A \\[1ex]
  \text{(incl-L)} & A \cap B <: A & \text{(incl-R)} & A \cap B <: B\\[1ex]
  \text{(mon)} & \inference{A <: A' & B <: B'}{A \cap B <: A' \cap B'} &
  \text{(trans)} & \inference{A <: B & B <: C}{A <: C} \\[2ex]
  \text{($\nu$)} & A \to B <: \nu \\[1ex]
  \text{($\to{-}\cap$)} & (A \to B) \cap (A \to C) <: A \to (B \cap C) &
  \text{($\eta$)} & \inference{A' <: A & B <: B'}
       {A \to B <: A' \to B'} \\[1ex]
  & A \sim B \defeq A <: B \text{ and } B <: A
\end{array}
\]
\caption{Types, subtyping, and equivalence}
\label{fig:intersection-types}
\end{figure}


Figure~\ref{fig:intersect-type-system} defines the intersection type
system.  It includes the introduction rule for intersection,
($\cap$-intro), which assigns expression $e$ the type $A \cap B$ if
$e$ can be typed with both $A$ and $B$.  The introduction rule for
$\nu$ says that every $\lambda$ has type $\nu$, even if the body of
the $\lambda$ is ill typed!  The subsumpton rule (sub), as usual,
enables implicit up-casts with respect to subtyping.  The rules for
variables (var), $\lambda$ abstraction ($\to$-intro), and application
($\to$-elim), are the same as in the simply-typed $\lambda$-calculus.
The (nat) rule is typical of singleton types.

\begin{figure}
\begin{gather*}
\text{(var)}
\inference{x:A \in \Gamma}
          {\Gamma \vdash x : A}
\qquad
\text{(nat)}
\inference{}{\Gamma \vdash n : n}
\qquad
\text{($\nu$-intro)}
\inference{}
          {\Gamma \vdash \LAM{x} e : \nu}
\\[2ex]
\text{($\to$-intro)}
\inference{\Gamma,x:A \vdash e : B}
          {\Gamma \vdash \LAM{x} e : A \to B}
\qquad
\text{($\to$-elim)}
\inference{\Gamma \vdash e_1 : A \to B & \Gamma \vdash e_2 : A}
          {\Gamma \vdash e_1 \APP e_2 : B}
\\[2ex]
\text{($\cap$-intro)}
\inference{\Gamma \vdash e : A & \Gamma \vdash e : B}
          {\Gamma \vdash e : A \cap B}
\qquad
\text{(sub)}
\inference{\Gamma \vdash e : A & A <: B}
          {\Gamma \vdash e : B}
\end{gather*}

\caption{An Intersection Type System}
\label{fig:intersect-type-system}
\end{figure}


\begin{proposition}[Subject Reduction and Expansion]\ 
  \begin{itemize}
  \item If $e \longrightarrow e'$ and $\Gamma \vdash e : A$,
    then $\Gamma \vdash e' : A$.
  \item If $e \longrightarrow e'$ and $\Gamma \vdash e' : A$,
    then $\Gamma \vdash e : A$.
  \end{itemize}
\end{proposition}


The intersection type system can be made into a $\lambda$-model
similar to the way we made $E$ into a $\lambda$-model by constructing
$E'$. Subtyping plays the same role as $\sqsubseteq$, but inverted.
So instead of working with ideals, we work with filters.

\begin{definition}[Filter]
  A subset $S$ of types $\mathbb{T}$ is a \emph{filter} if it is
  closed with respect to subtyping and intersection. That is,
  \begin{itemize}
  \item if $A \in S$ and $A <: B$, then $B \in S$, and
  \item if $A \in S$ and $B \in S$, then $A \cap B \in S$.
  \end{itemize}
  The set of all filters is $\mathbb{F}$.
  We define the upward closure of a set $\UP{S}$ as follows.
  \begin{gather*}
    \inference{A \in S}
              {A \in \UP{S}} 
    \quad
    \inference{A \in \UP{S} & A <: B}
              {B \in \UP{S}}
    \quad
    \inference{A \in \UP{S} & B \in \UP{S}}
              {A \cap B \in \UP{S}}
  \end{gather*}
\end{definition}


\marginnote{Is the upward closure really needed in the definition of
  application? We are missing that for $E$ and $E'$. TODO: mechanize
  this $\lambda$-model stuff to make sure one way or the
  other. -Jeremy}
\begin{definition}
  The \emph{filter $\lambda$-structure} is the triple $\langle
  \mathbb{F}, \cdot, F\rangle$ where
\begin{align*}
  \_ \cdot \_ & : \mathbb{F} \times \mathbb{F} \to \mathbb{F}\\
  S_1 \cdot S_2 &\defeq \UP{\{ B \mid \exists A \in S_2, A \to B \in S_1 \}}\\
  \Gamma \models \rho &\defeq \forall x\of B \in \Gamma, B \in \rho(x) \\
  F &: \mathbb{E} \to (\VAR \pto \mathbb{F}) \to \mathbb{F}\\
  F\,e\,\rho &\defeq \{ A \mid \exists \Gamma, \Gamma \models \rho \text{ and } \Gamma \vdash e : A\} 
\end{align*}
\end{definition}


\begin{lemma}
  The filter $\lambda$-structure $\langle \mathbb{F}, \cdot, F\rangle$
  is a $\beta_v$-model of the $\lambda$-calculus.
\end{lemma}

\begin{theorem}
  The filter $\lambda$-structure $\langle \mathbb{F}, \cdot, F\rangle$
  is a model of the CBV $\lambda$-calculus. That is,
  \[
  \text{if } \beta_v \vdash e = e', 
  \text{ then } \forall \rho, F\,e\,\rho = F\,e'\,\rho
  \]
\end{theorem}

%===============================================================================
\subsection{Graph models of $\lambda$-calculus $(T^{*}_C$, $D_A$, and $\mathcal{P}(\omega))$ }
\label{sec:graph-models}



%-------------------------------------------------------------------------------
\paragraph{The $T^{*}_C$ model of \citet{Plotkin:1972aa}}
\label{sec:t-c}


\newcommand{\EP}[0]{E_P}
\newcommand{\PSEM}[1]{\EP\,#1\,}

  Let $D$ range over finite sets of elements from $T_C$
  and $D^{*}$ range over possibly infinite sets.
  Figure~\ref{fig:t-c} defines the semantics.
  

\begin{figure}[tbp]
\begin{align*}
  T_C &= C + \mathcal{P}_f(T_C) \times \mathcal{P}_f(T_C) \\
  T^{*}_C &= \mathcal{P}(T_C) \\
  \uplambda^P &: [T^{*}_C \to T^{*}_C] \to T^{*}_C \\
  \uplambda^P f &= \{ (D,D') \mid D' \subseteq f\,D \} \\
  - \cdot_P - &: T^{*}_C \to T^{*}_C \to T^{*}_C \\
  D^{*}_1 \cdot_P D^{*}_2 &\defeq 
     \bigcup\{ D'\mid \exists D, (D,D') \in D^{*}_1, D \subseteq D^{*}_2\}\\[1ex]
%% \PSEM{ n }\rho &= \{ n \} \\
%% \PSEM{ e_1 \oplus e_2 }\rho &= \{  n_1 \oplus n_2 \mid 
%%    n_1 \in \PSEM{ e_1 }\rho \land n_2 \in \PSEM{ e_2 }\rho \} \\
\EP&: \mathbb{E} \to (\VAR\pto T^{*}_C) \to T^{*}_C\\
\PSEM{ x }\rho &= \rho(x) \\
%\PSEM{ (\LAM{x} e) }\rho &= 
%  \{ (D,D') \mid  D' \subseteq \PSEM{ e }[x\mapsto D]\rho \} \\
\PSEM{ (\LAM{x} e) }\rho &= \uplambda^P (\uplambda D^{*}.\, \PSEM{e}[x\mapsto D^{*}]\rho) \\
\PSEM{ (e_1\,e_2) }\rho &= (\PSEM{ e_1 }\rho) \cdot_P (\PSEM{ e_2 }\rho)
\end{align*}
\caption{The $T^{*}_C$ model}
\label{fig:t-c}
\end{figure}


%-------------------------------------------------------------------------------
\paragraph{The $D_A$ model of \citet{Engeler:1981aa}}
\label{sec:d-a}

\newcommand{\EE}[0]{E_E}
\newcommand{\ESEM}[1]{\EE\, #1 \,}


  Let $d$ range over elements of $B_A$.
  Let $D$ range over finite sets of elements from $B_A$ and $D^{*}$
  range over possibly infinite sets.  Figure~\ref{fig:d-a} gives the
  semantics, following the presentation of
  \citet{barendregt84:_lambda_calculus} (Section 5.4).
  

\begin{figure}[tbp]
\begin{align*}
  B_A &= A + \mathcal{P}_f(B_A) \times B_A \\
  D_A &= \mathcal{P}(B_A) \\
   \uplambda^E &: [D_A \to D_A] \to D_A \\
   \uplambda^E f &= \{ (D,d') \mid d' \in f\, D \} \\
   - \cdot_E - &: D_A \to D_A \to D_A \\
   D^{*}_1 \cdot_E D^{*}_2 &= \{ d' \mid \exists D, D \subseteq D^{*}_2 \land (D,d') \in D^{*}_1 \} \\[1ex]
\EE &: \mathbb{E} \to (\VAR\pto D_A) \to D_A\\
\ESEM{ x }\rho &= \rho(x) \\
\ESEM{ (\LAM{x} e) }\rho &= \uplambda^E (\uplambda D^{*}.\, \ESEM{e}[x\mapsto D^{*}]\rho)\\
\ESEM{ (e_1\,e_2) }\rho &= (\ESEM{ e_1 }\rho) \cdot_E (\ESEM{ e_2 }\rho) 
\end{align*}
\caption{The $D_A$ model}
\label{fig:d-a}
\end{figure}


$D_A$ ordered by set inclusion $\subseteq$ is a cpo.


\begin{definition}
  A cpo $D$ is \emph{reflexive} if $[D \to D]$ is a retract of $D$.
  That is to say, there are continuous maps $F:D \to [D \to D]$ and
  $G: [D \to D] \to D $ such that for any $f \in [D \to D]$,
  \[
    (F \circ G)\, f = f
  \]
\end{definition}


\begin{lemma}
  $D_A$ is a reflexive cpo by choosing $F = \uplambda x y.\, x \cdot_E
  y$ and $G= \uplambda^E$. Therefore $D_A$ is a
  $\lambda$-model.
\end{lemma}
\begin{proof}
  We need to show that $(F \circ G)\,f = f$
  for any $f \in [D_A \to D_A]$.
  \begin{align*}
    (F \circ G)\,f &= \uplambda x. (\uplambda^E f) \cdot_E x\\
    &= \uplambda x. \{ d' \mid \exists D, D \subseteq x \land d' \in f\,D \}\\
    &= \uplambda x. \bigcup \{ f\,D \mid D \subseteq x \} \\
    &= \uplambda x.\, f\, x  & \text{by continuity of } f \\
    &= f
  \end{align*}
\end{proof}


\noindent Resources about $D_A$:
\begin{itemize}
\item \citet{barendregt84:_lambda_calculus} (Section 5.4),
\item \citet{Gunter:1992aa} (Section 8.1), and
\item \citet{Engeler:1981aa}.
\end{itemize}


%-------------------------------------------------------------------------------
\paragraph{The $\mathcal{P}(\omega)$ model of \citet{Scott:1976lq}}
\label{sec:p-omega}


%% \marginnote{
%% \begin{tabular}{ll}
%% Reading: & \citet{barendregt84:_lambda_calculus} Sec. 18.1\\
%%          & \citet{Scott:1976lq}
%% \end{tabular}
%% }

Figure~\ref{fig:p-omega} defines a semantics for the
$\lambda$-calculus using the $\mathcal{P}(\omega)$ model of
\citet{Scott:1976lq}.  However, here we use the formulation of
$\mathcal{P}(\omega)$ given by \citet{barendregt84:_lambda_calculus}.

\newcommand{\EPo}[0]{E_S}
\newcommand{\POSEM}[1]{\EPo{}\,#1\,}


\begin{figure}
\begin{align*}
     & m,n \in \NAT \\
\langle n,m \rangle &= \frac{1}{2}(n + m)(n+m+1) + m \\
e_n &= \{ k_0, k_1, \ldots, k_{m-1} \} \quad \text{where }
    k_i < k_{i+1} \text{ and } n = \sum_{i<m} 2^{k_i}\\[2ex]
\EPo{} & : \mathbb{E} \to (\VAR \pto \mathcal{P}(\NAT)) \to \mathcal{P}(\NAT) \\
\POSEM{x}\rho &= \rho\,x \\  
\POSEM{(\LAM{x}e)}\rho &= \{ \langle n, m \rangle \mid m \in \POSEM{e}[x\mapsto e_n]\rho \} \\
\POSEM{(e_1\,e_2)}\rho &= \{ m \mid \exists e_n. \, 
   \langle n,m \rangle \in \POSEM{e_1}\rho
   \text{ and } e_n \subseteq \POSEM{e_2}\rho \}
\end{align*}
\caption{$\lambda$-calculus in $\mathcal{P}(\omega)$}
\label{fig:p-omega}
\end{figure}



%===============================================================================
\subsection{Untyped as a uni-typed}

\[
\begin{array}{lrl}
 \text{types} & A,B ::= & \mathit{nat} \mid A \to B \mid A \times B \mid \mathit{dyn} \\
 \text{type tag}  & t ::= & \mathit{fun} \mid \mathit{nat} \\
 \text{coercion} & c ::= & t! \mid t? \\
 \text{expr.}& e \in\mathbb{E} ::=& n \mid e + e \mid e \times e \mid x \mid \LAM{x \of A} e \mid (e \, e) \mid e \langle c \rangle
\end{array}
\]

\newcommand{\uni}[1]{U(#1)}
\newcommand{\coerce}[2]{#1\langle #2\rangle}

\begin{align*}
  \uni{n} &= \coerce{n}{\mathit{nat}!} \\
  \uni{e_1 + e_2} &= \coerce{(\coerce{\uni{e_1}}{\mathit{nat}?} +
    \coerce{\uni{e_2}}{\mathit{nat}?})}{\mathit{nat}!} \\
  \uni{x} &= x \\
  \uni{\LAM{x} e} &= \coerce{(\LAM{x\of \mathit{dyn}} \uni{e})}{\mathit{fun}!}\\
  \uni{e_1 \APP e_2} &= 
     \coerce{\uni{e_1}}{\mathit{fun}?} \APP \uni{e_2}
\end{align*}

\noindent Equational theory: STLC's + 
\[
\begin{array}{lc}
  & \coerce{\coerce{e}{t!}}{t?} = e
\end{array}
\]


\begin{theorem}
If $\vdash e = e'$, then $\vdash \uni{e} = \uni{e'}$.
\end{theorem}
\begin{proof}
The proof is by induction on the derivation of $\vdash e = e'$.
\begin{itemize}
\item[Case] $\vdash (\LAM{x}e_1)\APP e_2 = [ x := e_2 ]e_1$
  \begin{align*}
    \uni{(\LAM{x}e_1)\APP e_2}
        &= \coerce{(\uni{\LAM{x}e_1})}{\mathit{fun}?} \APP \uni{e_2}\\
        &= \coerce{\coerce{(\LAM{x}\uni{e_1})}{\mathit{fun}!}}{\mathit{fun}?} \APP \uni{e_2}\\
        &= (\LAM{x}\uni{e_1}) \APP \uni{e_2}\\
        &= [x:=\uni{e_2}]\uni{e_1} \\
        &= \uni{[x:=e_2]e_1} 
  \end{align*}
\item[Case $\vdash n_1 + n_2 = n_3$ where $n_3$ is $n_1 + n_2$]
\begin{align*}
  \uni{n_1+n_2} &= 
     \coerce{(\coerce{\coerce{n_1}{\mathit{nat}!}}{\mathit{nat}?} +
              \coerce{\coerce{n_2}{\mathit{nat}!}}{\mathit{nat}?})}{\mathit{nat}!} \\
  &= \coerce{(n_1 + n_2)}{\mathit{nat}!} \\
  &= \coerce{n_3}{\mathit{nat}!} \\
  &= \uni{n_3} 
\end{align*}
\item[TODO]
\end{itemize}
\end{proof}

%===============================================================================
\subsection{$D_\infty$ model of $\lambda$-calculus}
\label{sec:D-infinity}

UNDER CONSTRUCTION



\clearpage
\pagebreak

\section*{Answers to Exercises}

\shipoutAnswer

\clearpage
\pagebreak

\bibliographystyle{plainnat}
\bibliography{all}

\end{document}


%===============================================================================
\section{Records and Variants}

\section{Functions}

% STLC

Syntax
\[
\begin{array}{lrcl}
\text{expressions} & e & ::= & \ldots \mid x \mid \LAM{x\of T} e \mid e \APP e
\end{array}
\]


evaluation orders (specification)
\begin{itemize}
\item full $\beta$
\item CBV
\item CBN
\end{itemize}

\section{Recursive Functions}

% PCF
% statically typed, lambda, fix, nat, bool, if, primitives


\section{Dynamic Typing}


\section{The $\lambda$-calculus}

Syntax
\[
\begin{array}{lrcl}
\text{expressions} & e & ::= & \ldots \mid x \mid \LAM{x} e \mid e \APP e
\end{array}
\]



\section{Pointers}

\section{Miscellaneous}

A lub for a set $S$ can only exist of all the elements in $S$ are
consistent with one another.
%
\marginnote{$\{3\mapsto 8\}$ and $\{3 \mapsto 9\}$ are inconsistent.}
%
Consistency can be characterized purely in terms of the ordering
relation.

\begin{definition}
A subset $S$ of a partial order $(L,\sqsubseteq)$ is
\textbf{\emph{consistent}} if every pair of elements $x$ and $y$ in
$S$ has a common upper bound, i.e.  there exists $z \in L$ such $x
\sqsubseteq z$ and $y \sqsubseteq z$.
\end{definition}

In the poset of partial functions $(A \pto B, \subseteq)$,
every consistent subset $S$ has a lub, the function whose graph is the
union of all the graphs in $S$.

%
%% A partial order in which all consistent subsets have a least upper
%% bound is \textbf{\emph{consistently complete}}~\citep{Plotkin:1978aa}
%% (aka. \emph{bounded complete}~\citep{Gunter:1992aa}).

\begin{marginfigure}
\centering\xymatrix@=10pt{
 & \{ 1,2,3\} \ar@{-}[ddl]\ar@{-}[dr]\\
 & & \{ 2,3 \} \ar@{-}[dl]\ar@{-}[d]\\
\{1\}\ar@{-}[dr] & \{ 2 \} \ar@{-}[d]& \{ 3 \}\ar@{-}[dl]\\
   & \emptyset & 
} 
\caption{A directed subset of $(\mathcal{\mathbb{N}},\subseteq)$.}
\end{marginfigure}

However, a slightly different property, directedness, is used in the
fixed point theorem. (I am trying to find out why.) A directed set is
similar to a consistent set, but we only require an upper bound for
each pair in $S$ and the bound must itself be in $S$.

\begin{marginfigure}
\centering\xymatrix@=10pt{
 & & \{ 2,3 \} \ar@{-}[dl]\ar@{-}[d]\\
\{1\}\ar@{-}[dr] & \{ 2 \} \ar@{-}[d]& \{ 3 \}\ar@{-}[dl]\\
   & \emptyset & 
}
\caption{A subset of $(\mathcal{\mathbb{N}},\subseteq)$ that is
not directed.}
\end{marginfigure}


\begin{definition}
  Given a poset $(L,\sqsubseteq)$, a subset $S$ of $L$ is
  \textbf{\emph{directed}} if every pair of elements in $S$ has an
  upper bound in $S$.
\end{definition}

The sequence of approximations $\bot \sqsubseteq F(\bot) \sqsubseteq
F^2(\bot) \sqsubseteq \cdots$ is directed, any totally ordered subset
is, because the upper bound of two elements is just the larger
element.

\begin{marginfigure}
\centering\xymatrix@=10pt{
    & a \ar@{-}[dl]\ar@{-}[dr]& \\
  b &   & c
}
\caption{The subset $\{b,c\}$ is consistent but not directed.
  The subset $\{a,b,c\}$ is both consistent and directed.}
\end{marginfigure}

\begin{proposition}
  A finite directed set $S$ is also consistent.
\end{proposition}
\begin{proof}
  By induction on $S$. If $S=\emptyset$, then it vacuously consistent.
  Otherwise, remove a minimal element $x$ from $S$.  Then $S'=S -
  \{x\}$ is still directed, so $S'$ is consistent by the induction
  hypothesis, say with bound $z$. Then because $S$ is directed, for
  every $y \in S'$, the pair $x$ and $y$ have a bound $w$ in $S'$ (the
  bound couldn't be $x$), and $w \sqsubseteq z$ (because $S'$ is
  consistent) so $x \sqsubseteq z$.  Thus, $S$ is consistent.
\end{proof}

Generalizing from the poset of partial functions, one might think that
there is always a least upper bound of consistent or directed sets.
However, this is not always the case because their may not be enough
elements within the poset. For example, consider the poset
$(\NAT,\leq)$. The sequence
\[
  0 \leq 1 \leq 2 \leq 3 \leq \cdots
\]
does not have a lub in $(\NAT,\leq)$ because there is no one
natural number that is greater than all the rest. Of course, if we
move to the poset $(\NAT \cup \{\infty\},\leq)$, where $n \leq
\infty$ for any $n \in \NAT$, then $\infty$ is the lub of the
above sequence. So in the abstract setting of partial orders, one must
separately add the requirement that there exists a lub for any
directed subset.

\begin{marginfigure}
\centering\xymatrix@=10pt{
 a \ar@{-}[d]\ar@{-}[drr] &          & b \ar@{-}[d] \ar@{-}[dll]\\
 c \ar@{-}[dr]&          & d \ar@{-}[dl]\\
         & e 
}  \\[2ex]
\caption{A dcpo with a consistent set $\{c,d\}$ that has no
  lub~\citep{Plotkin:1978aa}.}
\end{marginfigure}

\begin{definition}
  A \textbf{\emph{directed-complete partial order (dcpo)}} has a least
  upper bound for every directed subset.
\end{definition}

\begin{proposition}[Partial functions are dcpos]
  Given any two sets $A$ and $B$, the partial order $(A \pto B,
  \subseteq)$, is a dcpo.
\end{proposition}

\begin{theorem}[Fixed Point Theorem for Dcpos]\label{thm:fixed-point}
Suppose $(L,\sqsubseteq)$ is a dcpo with a least element $\bot$, and
let $F:L\to L$ be a continuous function. Then $F$ has a least fixed
point, written $\mathrm{fix}\,F$, which is the least upper bound of
the ascending chain of $F$:
\[
  \mathrm{fix}\,F = \bigsqcup \{ F^n(\bot) \mid n \in \NAT \}
\]
\end{theorem}
\begin{proof}
We first prove that $\mathrm{fix}\,F$ is a fixed point of $F$.
\begin{align*}
  F(\mathrm{fix}\,F) &= F(\bigsqcup \{ F^n(\bot) \mid n \in \NAT \})\\
  &= \bigsqcup\{ F(F^n(\bot)) \mid n \in \NAT \} & \text{by continuity}\\
  &= \bigsqcup\{ F^{n+1}(\bot)) \mid n \in \NAT \} \\
  %% &= \bigsqcup \{ F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  %% &= \bigsqcup \{ F^0(\bot), F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  &= \bigsqcup\{ F^n(\bot)) \mid n \in \NAT \} 
  & \text{because } F^0(\bot) = \bot \sqsubseteq F^1(\bot)  \\
  &= \mathrm{fix}\,F
\end{align*}
Next we prove that $\mathrm{fix}\,F$ is the least of the fixed points
of $F$. Suppose $e$ is an arbitrary fixed point of $F$. By the
monotonicity of $F$ we have $F^i(\bot) \sqsubseteq F^i(e)$ for all
$i$.  And because $e$ is a fixed point, we also have $F^i(e) = e$, so
$e$ is an upper bound of the ascending chain, and therefore
$\mathrm{fix}\,F \sqsubseteq e$.
\end{proof}


%%  LocalWords:  implementers
